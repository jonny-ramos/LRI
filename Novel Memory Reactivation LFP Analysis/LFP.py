import numpy as np
import matplotlib.pyplot as plt
import sys
import datetime
import random
import math
from numpy.fft import fft, ifft
from numpy.linalg import inv
from numpy.linalg import cholesky
from scipy.signal import butter, filtfilt, hilbert, decimate
from scipy import signal
from scipy import stats

def get_data_by_rat(DATA, rat):
    for key in DATA.keys():                 # DATA is global. data should be loaded in the form of a dict.
        if rat in key and 'PFC' in key:     # where keys are (descriptive) file names
            PFC = DATA[key]                 # and values are arrays containing timeseries data for that session
        if rat in key and 'HPC' in key:
            HPC = DATA[key]
        if rat in key and 'BLA' in key:
            BLA = DATA[key]
        if rat in key and 'REF' in key:
            REF = DATA[key]

    return PFC, BLA, HPC, REF           # in that order!

def norm_timeseries(DATA, rat, ref_side='L', HPC_ch=True, BLA_ch=True):
    '''
    ref_side is either 'L' or 'R' and denotes which side of the cerebellum data should be referrenced to.
    default to 'L'

    HPC_ch, BLA_ch denote whether or not we expect to have recordings HPC and BLA. True if there is data
    from respective region, False otherwise.
    defualt to True.
    '''
    def check_chLen(R, rat, ref_side):
        # R is a list of region variables generated by get_data_by_rat.
        # i.e. R = [PFC, BLA, HPC, REF]

        reg_lens = []
        for reg in R:
            ch_id = list(reg.keys())
            timeseries_len = [len(reg[ch]['timeseries']) for ch in ch_id]

            reg_lens.append(timeseries_len)

        reg_lens = [item for sublist in reg_lens for item in sublist]

        if len(np.unique(reg_lens)) == 1:
            pass

        elif len(np.unique(reg_lens)) > 1:
            min_len = np.min(reg_lens)

            #==================================================================#
            with open('trial_len_mismatch.txt', 'w') as sys.stdout:
                print(f'{rat} contains chs of unequal len\n')
                print('comparing cue_light len, timestamps across regions:\n')
                for reg in R:
                    ch_key = list(reg.keys())[0]
                    print(f'{ch_key} len: {len(reg[ch_key]["timeseries"])}')    # note to self: use " " inside f strings
                    print(f'{ch_key} cue_light: {reg[ch_key]["cue_light"]}\n')
                    #print(f'{ch_id[0]} lever: {reg[ch_id[0]]["lever"]}')
            #==================================================================#

            for reg in R:
                ch_id = list(reg.keys())
                for ch in ch_id:
                    reg[ch]['timeseries'] = reg[ch]['timeseries'][:int(min_len)]

        for key in REF.keys():
            if ref_side == 'L':
                if '9' in key or '41' in key:
                    REF_chID = key
            if ref_side == 'R':
                if '8' in key or '40' in key:
                    REF_chID = key

        REF_timeseries = REF[REF_chID]['timeseries']

        return R, REF_timeseries

    def norm_perCh(R, REF_timeseries):
        for reg in R:
            ch_id = list(reg.keys())

            for ch in ch_id:
                timeseries = reg[ch]['timeseries']
                referenced = timeseries - REF_timeseries

                reg[ch]['referenced'] = referenced

        return R

    # the main part of the fn
    PFC, BLA, HPC, REF = get_data_by_rat(DATA, rat)
    R = [PFC, BLA, HPC, REF]

    if HPC_ch == False:
        R.remove(HPC)

    elif BLA_ch == False:
        R.remove(BLA)

    elif HPC_ch == False and BLA_ch == False:
        R.remove(HPC)
        R.remove(BLA)

    elif HPC_ch == True and BLA_ch == True:
        pass

    R, REF_timeseries = check_chLen(R, rat, ref_side=ref_side)
    R_norm = norm_perCh(R, REF_timeseries)

    return R_norm

def slice_by_side(reg, s_pre, s_post, RAT_ID, rewarded=True):
    '''
    takes a list of region dictionaries
    '''
    # ok we need to know first: is the ch recorded from L or R side?
    # and second: how many samples +/- from the TTL timestamp do we need to cut out?
    # let's start by defning smaller functions to accomplish each of these tasks.

    def ch_hemisphere_ID(ch_ID):
        ch_n = int(ch_ID.split('_')[1])

        # see ch pinout mapping
        Rside = list(range(25,33))+list(range(1,9))
        Rside = set(Rside + list(np.array(Rside)+32))

        Lside = list(range(9,25))
        Lside = set(Lside + list(np.array(Lside)+32))

        if ch_n in Rside:
            return 'R_side'

        if ch_n in Lside:
            return 'L_side'

    def slice_epochs(reg, n_pre, n_post):
        # n_pre, n_post are the number of seconds before event time stamp
        # and the number of seconds post event timestamp respectively

        fs = 2000 # sampling rate hard coded in here because I've already downsampled to 2kHz

        ch_id = list(reg.keys())

        light = set(reg[ch_id[0]]['cue_light'])
        lever = set(reg[ch_id[0]]['lever'])

        rewarded_ts = sorted(list(light))
        unrewarded_ts = sorted(list(lever - light))[:-1]

        # with open('timestamps.txt', 'w') as sys.stdout:
        #     print(f'rewarded_ts: {rewarded_ts}')
        #     print(f'unrewarded_ts: {unrewarded_ts}')

        def filter_timestamps(timestamps, cutoff):
            filt_t = []
            for i in range(len(timestamps)):
                if i == 0:
                    filt_t.append(timestamps[i])
                    current_t = timestamps[i]

                dt = timestamps[i] - current_t

                if dt > cutoff:
                    filt_t.append(timestamps[i])
                    current_t = timestamps[i]

            return filt_t

        rewarded_ts = filter_timestamps(rewarded_ts, n_pre+n_post)
        unrewarded_ts = filter_timestamps(unrewarded_ts, n_pre+n_post)

        rewarded_epochs_bych = []
        for ch in ch_id:
            timeseries = reg[ch]['timeseries']
            epochs = [timeseries[int(ts*fs - n_pre*fs):int(ts*fs + n_post*fs)] for ts in rewarded_ts]

            rewarded_epochs_bych.append((ch, epochs))

        unrewarded_epochs_bych = []
        for ch in ch_id:
            timeseries = reg[ch]['timeseries']
            epochs = [timeseries[int(ts*fs - n_pre*fs):int(ts*fs + n_post*fs)] for ts in unrewarded_ts]

            unrewarded_epochs_bych.append((ch, epochs))

            # with open('timeseries_len.txt', 'w') as sys.stdout:
            #     print(len(timeseries))

        d_rewarded_epochs = dict(rewarded_epochs_bych)
        d_unrewarded_epochs = dict(unrewarded_epochs_bych)

        return d_rewarded_epochs, d_unrewarded_epochs

    #### the main part of the function
    reg_Rside = {}
    reg_Lside = {}

    for rat in reg:
        # with open('rat_reg_test.txt', 'w') as sys.stdout:
        #     print(reg)
        rat_reg = reg[rat]

        (d_rewarded, d_unrewarded) = slice_epochs(rat_reg, s_pre, s_post)    # 1.5s before, 1.5s after

        if rewarded == True:
            d_epochs = d_rewarded

        else:
            d_epochs = d_unrewarded

        for key in d_epochs.keys():
            if ch_hemisphere_ID(key) == 'R_side':

                reg_Rside[rat] = d_epochs[key]
                break                                   # I only need one for each side
                                                        # for LFPs it doesn't super matter which one
        for key in d_epochs.keys():                     # I've looked enough times to convince myself that
            if ch_hemisphere_ID(key) == 'L_side':       # they all have roughly the same LFP information.

                reg_Lside[rat] = d_epochs[key]
                break

    return reg_Rside, reg_Lside

def flatten_list(lst):
    # just for convenience, flattens list of lists into a single list
    return [e for sublist in lst for e in sublist]

def rms(x):
    rms = np.sqrt(np.mean(np.array(x) ** 2)) # the sqrt of the mean of squares
    return(rms)

def rms_artifact_ID(trial, stepsize, sampsize, rmsthresh):
    '''
    Takes a signal data and looks for artifacts by taking a section of len
    sampsize and calculating the rms. Then slides one step down, defined by
    stepsize, and calculates the rms again. Repeats this for the length of the
    signal. Then compares each rms value to rmsthresh times the medain.

    Returns a list of rms values across the whole signal.
    Returns an array of booleans that were above the defined rms threshold.

    trial is a list containing signal data.
    stepsize is an int.
    sampsize is an int.
    rmsthresh can be int or float.
    '''
    rmss = []
    for i in range(int((len(trial)/stepsize))):
        trial_samp = trial[i*stepsize:i*stepsize+sampsize]
        trial_samp_rms = rms(trial_samp)

        rmss.append(trial_samp_rms)

    # without knowing beforehand which sections of data are a clean baseline,
    # if we assume that there are only a few artifacts per trial, we can
    # guess that the median rms will likely be a reasonable baseline.
    rms_baseline = np.median(rmss)

    artifact_bools = rmss > rmsthresh*rms_baseline

    # returning an array of booleans here because we will need to
    # add arrays of booleans from PFC and HPC together
    return(artifact_bools)

def get_artifacts(R, step, samp, thresh):
    '''
    just a small wrapped fn to shorten up some artifact detection code.

    R is a dictionary of trials for that region where the keys are rat_ns
    and the values are a list of trials (each a list of floats) for that rat.

    artifacts is a dictionary of artifacts per region where the keys are rat_ns
    and the values are a list of trial artifacts (each an ARRAY of booleans) for that rat.
    '''
    artifacts = {}

    for rat in R:
        a = [rms_artifact_ID(trial, stepsize=step, sampsize=samp, rmsthresh=thresh) for trial in R[rat]]
        artifacts[rat] = a

    return artifacts

def filter_artifacts(trials, artifact_filter):
    '''
    Takes a list of trials (each trial is a list of values) and a list of artifact filters and
    removes trials which contained any artifact via boolean indexing.

    some notes:
    The artifact filter is a list of booleans that is the same shape as the list of trials.
    Each trial is matched (in the same order) to a list of booleans in artifact_filter (meaning
    we can access the corresponding filter with the same index for each trial)
    I can tell if any trial had an artifact removed because it will be shorter than the original
    trial, so only return cleaned_trials of equal len to trial[i].
    '''
    if artifact_filter is None:
        return None

    cleaned = []
    for i in range(len(trials)):
        cleaned_trial = np.array(trials[i])[~artifact_filter[i]]

        if len(cleaned_trial) == len(trials[i]):    # 2000 is fs, hardcoded here bc i never expect anything else.
            cleaned.append(cleaned_trial)

    return cleaned

def sum_artifact_dicts(artifact1, artifact2):
    '''
    small function to make addition make sense between two dictionaries whose
    keys are rat_ns and whose values are a list of arrays of booleans

    this is to ensure that when we take trials for multiregional analyses, we
    only take trials that were "clean" from both regions.
    '''
    summed_artifact_dict = {}
    for rat in artifact1:
        afilt1 = artifact1[rat]
        afilt2 = artifact2[rat]         # i expect these two dicts to have the same keys.

        artifact_sum = [a1 + a2 for a1, a2 in zip(afilt1, afilt2)]   # artifacts are stored as arrays so we can just add them

        summed_artifact_dict[rat] = artifact_sum

    return summed_artifact_dict

def curate_trials(cleaned_trials):
    '''
    Plots trials one at a time and prompts user to input Y or N to indicate whether
    or not that trial should be kept or rejected, Y to keep trial, N to reject trial.
    This is (hopefully) the final level of curation prior to running any analyses.
    Although the pipeline is relatively short now, only a two step process: First
    reject any trials which had artifacts (detected by params above) then visually
    curate from the remaining trials.
    Further preprocessing may include testing for stationarity, but I feel it is
    best to keep the preprocessing minimal to minimize skewing results and maintaining
    as straightforward an interpretationg as possible.
    '''
    sys.stdout = sys.__stdout__

    rats = list(cleaned_trials.keys())

    for rat in rats:
        print(f'\n{rat}\n')
        trial_ns = list(cleaned_trials[rat].keys())

        keep_trials = []
        for trial_n in trial_ns:
            plt.figure(figsize=(15,3))
            plt.plot(cleaned_trials[rat][trial_n])
            plt.pause(.001)

            keep_trial = input('Keep trial? Y/N: ')
            while keep_trial.lower() != 'y' and keep_trial.lower() != 'n':
                print('That was not a valid response. Please enter Y/N')
                keep_trial = input('Keep trial? Y/N: ')

            if keep_trial.lower() == 'y':
                keep_trials.append(True)
            elif keep_trial.lower() == 'n':
                keep_trials.append(False)

            plt.close()

        for trial_i, keep_trial in zip(trial_ns, keep_trials):
            if keep_trial == False:
                del cleaned_trials[rat][trial_i]

def ERP(trials):
    if not trials is None:
        erp = np.array([sum(sample) for sample in list(zip(*trials))]) / len(trials)
        return erp

    else:
        return None

def plot_ERP(erp, global_labels, local_labels):
    f, ax = plt.subplots(1,1)
    f.set_figheight(5)
    f.set_figwidth(7)

    fs = 2000
    time = np.arange(-int(len(erp)/2),int(len(erp))/2)
    title = ' '.join([global_labels, local_labels])

    ax.set_title(title)
    ax.plot(time, erp)

    f.savefig(' '.join([title, 'ERP.png']), dpi=600)

def compute_mwt(signal, fs, peak_freq, n):
    '''
    Takes a timeseries and computes morlet wavelet convolution.
    First, generates a wavelet of peak frequency = peak_freq, then takes fft of signal and fft of
    wavelet. Then takes ift of the product of fft of signal and fft of wavelet and cuts off the
    "wings" of convolution (1/2 length wavelet from either end). I know the length of our wavelet
    is an odd number of points (fs=2000) so it's more convenient to use floor division here.
    Lastly, extracts amplitude and phase from the result of convolution.

    Inputs:
    signal is a list of timeseries data
    sampling rat, peak_freq, n are int

    Outputs:
    the result of convolution is a list of complex coefficients
    extracted ampltidue is a list of real numbers
    extract phase is a list of angles given in radians
    '''

    sig = signal
    fs = fs

    # generating our wavelet
    f = peak_freq
    t = np.arange(-1, 1 + 1/fs, 1/fs)
    s = n/(2*np.pi*f)
    wavelet = np.exp(2*np.pi*1j*f*t) * np.exp(-t**2/(2*s**2))

    # fft params
    n_sig = len(sig)
    n_wavelet = len(wavelet)
    n_conv = n_wavelet + n_sig - 1
    n_half_wavelet = len(wavelet) // 2

    # convolultion
    sig_fft = fft(sig, n_conv)
    wavelet_fft = fft(wavelet, n_conv)
    conv_result = ifft(sig_fft * wavelet_fft) * (np.sqrt(s)/20) # scaling factor = np.squrt(s)/20
    conv_result = conv_result[n_half_wavelet:-n_half_wavelet]   # bc that's what mike used in the text

    return(conv_result) # removed amp and pha outputs for more straight forward list comprehension

def compute_mwt(signal, fs, peak_freq, n):
    '''
    Takes a timeseries and computes morlet wavelet convolution.
    First, generates a wavelet of peak frequency = peak_freq, then takes fft of signal and fft of
    wavelet. Then takes ift of the product of fft of signal and fft of wavelet and cuts off the
    "wings" of convolution (1/2 length wavelet from either end). I know the length of our wavelet
    is an odd number of points (fs=2000) so it's more convenient to use floor division here.
    Lastly, extracts amplitude and phase from the result of convolution.

    Inputs:
    signal is a list of timeseries data
    sampling rat, peak_freq, n are int

    Outputs:
    the result of convolution is a list of complex coefficients
    extracted ampltidue is a list of real numbers
    extract phase is a list of angles given in radians
    '''

    sig = signal
    fs = fs

    # generating our wavelet
    f = peak_freq
    t = np.arange(-1, 1 + 1/fs, 1/fs)
    s = n/(2*np.pi*f)
    wavelet = np.sqrt(1/(s*np.sqrt(np.pi)))*np.exp(2*np.pi*1j*f*t) * np.exp(-t**2/((2*s)**2))

    # fft params
    n_sig = len(sig)
    n_wavelet = len(wavelet)
    n_conv = n_wavelet + n_sig - 1
    n_conv_pwr2 = 2**(math.ceil(np.log2(np.abs(n_conv))))
    n_half_wavelet = len(wavelet) // 2

    # convolultion
    sig_fft = fft(sig, n_conv_pwr2)
    wavelet_fft = fft(wavelet, n_conv_pwr2)
    conv_result = ifft(sig_fft * wavelet_fft)[:n_conv]#* (np.sqrt(s)/20) # scaling factor = np.squrt(s)/20
    conv_result = conv_result[n_half_wavelet:-n_half_wavelet]   # bc that's what mike used in the text

    return(conv_result) # removed amp and pha outputs for more straight forward list comprehension

def average_nbym_matrices(m):
    if not m is None:
        sums = np.zeros_like(m[0])
        for i in np.arange(len(m)):
            sums = sums + m[i]
        mean = sums / len(m)
        return(mean)

    else:
        return None

def norm(means, baseline_mean):
    '''
    small norm fn for baseline_norm()
    '''
    mean_norm = []
    for i in range(len(baseline_mean)):
        norm = means[i] / baseline_mean[i]
        mean_norm.append(norm)
    return(np.array(mean_norm))

def baseline_norm(Tr_pow, s_pre, s_post, epoch_len, pre_bline):
    # calculate epoch and baseline indices from s_pre, s_post. I hate always doing
    # this by hand so let's just write it in terms of s_pre, s_post which are given
    '''
    short fn to slice baseline and epoch from power extracted from mwt.
    For each f (defined by params of convolution) takes the average value in
    baseline slice and divides every value in epoch slice by that value.
    '''
    fs = 2000
    epoch_samples = epoch_len * fs                              # the duration of our entire epoch we want to cut out
    epoch_start = int((s_pre * fs ) - (epoch_samples / 2))      # the sample (index) where our epoch begins
    epoch_end = int(epoch_samples) + epoch_start                # the sample (index) where our epoch ends (the start i + total epoch duration)
    baseline_end = int((epoch_samples / 2) - (pre_bline * fs))  # the sample, i where [0:i] will be used for baselining

    if not epoch_end < int(s_post * fs) + int(s_pre * fs):
        print('Epoch end index beyond trial duration. Check your slice indices.')
        sys.exit(1)

    # cutting off our "wings"
    Trpow = [[fs[epoch_start:epoch_end] for fs in epoch] for epoch in Tr_pow]

    # defining our baseline epochs
    Trbase = [[fs[0:baseline_end] for fs in epoch] for epoch in Trpow]

    # defining our signal epochs
    Trsig = [[fs[baseline_end:] for fs in epoch] for epoch in Trpow]

    # means of baselines
    Trbase_fmean = [np.array([freq.mean() for freq in epoch]) for epoch in Trbase]

    # normlize to baseline
    Tr_norm = []
    for i in range(len(Trsig)):
        trial_norm = norm(Trsig[i], Trbase_fmean[i])
        Tr_norm.append(trial_norm)

    return Tr_norm, epoch_len, pre_bline    # return epoch_len, pre_bline for plotting.

def spectrogram(trials, freqs, n_dict, s_pre, s_post, epoch_len, pre_bline):
    '''
    wrapped spectrogram function for high frequncy analysis.
    '''
    def _baseline_norm(Tr_pow, s_pre, s_post, epoch_len, pre_bline):
        # calculate epoch and baseline indices from s_pre, s_post. I hate always doing
        # this by hand so let's just write it in terms of s_pre, s_post which are given
        '''
        short fn to slice baseline and epoch from power extracted from mwt.
        For each f (defined by params of convolution) takes the average value in
        baseline slice and divides every value in epoch slice by that value.
        '''
        def _norm(means, baseline_mean, db=False):
            '''
            small norm fn for baseline_norm()
            '''
            mean_norm = []
            for i in range(len(baseline_mean)):
                if db == False:
                    norm = means[i] / baseline_mean[i]
                elif db == True:
                    norm = 10*np.log10(means[i] / baseline_mean[i])
                mean_norm.append(norm)

            return(np.array(mean_norm))

        fs = 2000
        epoch_samples = epoch_len * fs                              # the duration of our entire epoch we want to cut out
        epoch_start = int((s_pre * fs ) - (epoch_samples / 2))      # the sample (index) where our epoch begins
        epoch_end = int(epoch_samples) + epoch_start                # the sample (index) where our epoch ends (the start i + total epoch duration)
        baseline_end = int((epoch_samples / 2) - (pre_bline * fs))  # the sample, i where [0:i] will be used for baselining

        if not epoch_end < int(s_post * fs) + int(s_pre * fs):
            print('Epoch end index beyond trial duration. Check your slice indices.')
            sys.exit(1)

        # cutting off our "wings"
        Trpow = [[fs[epoch_start:epoch_end] for fs in epoch] for epoch in Tr_pow]

        # defining our baseline epochs
        Trbase = [[fs[0:baseline_end] for fs in epoch] for epoch in Trpow]

        # defining our signal epochs
        Trsig = [[fs[baseline_end:] for fs in epoch] for epoch in Trpow]

        # means of baselines
        Trbase_fmean = [np.array([freq.mean() for freq in epoch]) for epoch in Trbase]

        # normlize to baseline
        Tr_norm = []
        for i in range(len(Trsig)):
            trial_norm = _norm(Trsig[i], Trbase_fmean[i])
            Tr_norm.append(trial_norm)

        return Tr_norm, epoch_len, pre_bline

    if not trials == []:
        fs = 2000
        freqs = freqs

        mwt = [[compute_mwt(trial, fs=fs, peak_freq=f, n=n_dict[f]) for f in freqs] for trial in trials]
        pow = [(abs(np.array(epoch)))**2 for epoch in mwt]
        norm, epoch_len, pre_bline = baseline_norm(pow, s_pre, s_post, epoch_len, pre_bline)

        normavg = average_nbym_matrices(norm)

        return normavg

    else:
        return None

def plot_spectrogram(normavg, epoch_len, pre_bline, freqs, global_labels, local_labels):
    '''
    global labels should be something like 'ChABC_VR5'
    local labels should be something like 'PFC Lside, Rewarded'
    '''
    if not normavg is None:
        f, ax = plt.subplots(1,1)
        f.set_figheight(5)
        f.set_figwidth(7)

        fs = 2000
        freqs = freqs
        time = np.arange(-1*pre_bline, epoch_len/2, 1/fs)*1000     # *1000 to get into ms
        title = ' '.join([global_labels, local_labels])

        n = 50
        vmin = np.min(normavg)    #================== harded coded params =================#
        vmax = np.max(normavg)
        levels = np.linspace(vmin, vmax, n+1)



        cs = ax.contourf(time, freqs, normavg, levels=levels, cmap='magma')
        ax.set_title(title)
        ax.set(xlabel='Time (ms), press @ t=0')
        f.colorbar(cs, ax=ax, shrink=0.9)

        frange = f'{freqs.min()}-{freqs.max()} Hz'
        #f.savefig(' '.join([title, frange,'Spectrogram.png']), bbox_inches='tight', dpi=600)

        np.save(' '.join([title, frange,'Spectrogram.npy']), normavg, allow_pickle=True)
        plt.close()

    else:
        pass

def mwt_angle(trials, freqs, ns):
    '''
    small fn to quickly extract angle information from complex coefficients generated by
    wavelet convolution in the shape of our data.
    '''
    if not trials is None:
        angles = []
        for trial in trials:
            trial_angles = []

            for i in range(len(freqs)):
                # convolve
                mwt = compute_mwt(trial, 2000, peak_freq=freqs[i], n=ns[i])

                # extract angles
                trial_angles.append(np.angle(mwt))

            angles.append(trial_angles)

        return angles

    else:
        return None


def calc_ITPC(angles):
    '''
    small fn to quickly compute ITPC in the shape of our data (list of list of lists)
    '''
    if not angles is None:
        angles_T = np.transpose(angles)

        ITPCs = []
        for sig in angles_T:

            ITPC_byfreq = []
            for freq in sig:
                # compute ITPC
                ITPC_byfreq.append(np.abs(np.mean(np.exp(1j * freq))))

            ITPCs.append(ITPC_byfreq)

        ITPCs = np.transpose(ITPCs)
        return ITPCs

    else:
        return None

def ITPC(trials, s_pre, s_post, pre_bline, epoch_len, freqs): #epoch_len = 2
    '''
    wrapped ITCP method with hard coded params for low freq ITPCs with 1 s time
    window. edit here for different params.
    '''
    #freqs = np.logspace(np.lot10(4), np.log10(40),20)
    freqs=freqs
    ns = [6 + (i-4)*4/len(range(4,39)) for i in range(4,40)]

    fs = 2000
    epoch_samples = fs * epoch_len
    epoch_start = int((s_pre * fs) - (epoch_samples / 2))
    epoch_end = int(epoch_samples) + epoch_start
    baseline_end = int((s_pre * fs) - (pre_bline * fs))

    if not epoch_end < int(s_post * fs) + int(s_pre * fs):
        print('Epoch end index beyond trial duration. Check your slice indices.')
        sys.exit(1)

    angles = mwt_angle(trials, freqs, ns)
    #print(f'shape of mwt_angle: {np.shape(angles)}')

    angles = [[f[epoch_start:epoch_end] for f in trial] for trial in angles]
    #print(f'shape after slicing: {np.shape(angles)}')

    ITPCs = calc_ITPC(angles)

    ITPC_baseline_subtraction = []
    for itpc in ITPCs:
        baseline_end_i = int(baseline_end - epoch_start)
        baseline_avg = np.average(itpc[0: baseline_end_i])

        itpc_baseline_subtraction = np.array(itpc) - baseline_avg
        ITPC_baseline_subtraction.append(itpc_baseline_subtraction[baseline_end_i:])

    #print(f'shape of calc_ITPC: {np.shape(ITPCs)}')

    return ITPC_baseline_subtraction

def plot_ITPCs(itpc, epoch_len, pre_bline, freqs, global_labels, local_labels):
    # note: pre_bline and epoch_len are in SECONDS.

    if not np.shape(itpc) == (0,):

        f, ax = plt.subplots(1,1)
        f.set_figheight(5)
        f.set_figwidth(7)
        title = ' '.join([global_labels, local_labels])

        fs = 2000
        freqs = freqs
        time = np.arange(-1*pre_bline, epoch_len/2, 1/fs)*1000  # *1000 to get into ms
        n_levels = 80
        levels = np.linspace(np.min(itpc), 0.5, n_levels+1)

        cs = ax.contourf(time, freqs, itpc, n_levels, levels=levels, vmin=0, vmax=0.5)
        ax.set_title(title)
        ax.set(xlabel='Time (ms), press @ t=0', ylabel='Frequency (Hz)')
        f.colorbar(cs, ax=ax, shrink=0.9, ticks=[0, 0.1, 0.2, 0.3, 0.4, 0.5])

        frange = f'{freqs.min()}-{freqs.max()} Hz'
        f.savefig(' '.join([title, frange, 'ITPC.png']), bbox_inches='tight', dpi=600)
        plt.close()

##### Inter Site Phase CLustering
def calc_ISPC(R1_angles, R2_angles):
    R1_angles_T = np.transpose(R1_angles)
    R2_angles_T = np.transpose(R2_angles)

    # finding our angle differences
    R1_R2_angles_diff_T = R1_angles_T - R2_angles_T

    R1_R2_ISPCs = []

    for sig in R1_R2_angles_diff_T:
        ISPC_byfreq = []

        for freq in sig:
            # compute ISPC
            ISPC_byfreq.append(np.abs(np.mean(np.exp(1j * freq))))

        R1_R2_ISPCs.append(ISPC_byfreq)

    R1_R2_ISPCs = np.transpose(R1_R2_ISPCs)

    return R1_R2_ISPCs

def ISPC(R1_trials, R2_trials, s_pre, s_post, pre_bline, epoch_len, freqs, ns): #epoch_len = 2
    '''
    wrapped ISPC method with hard coded params for low freq ISPCs with 1 s time
    window. edit here for different params.
    '''
    #freqs = np.logspace(np.lot10(4), np.log10(40),20)
    freqs=freqs

    fs = 2000
    epoch_samples = fs * epoch_len
    epoch_start = int((s_pre * fs) - (epoch_samples / 2))
    epoch_end = int(epoch_samples) + epoch_start
    baseline_end = int((s_pre * fs) - (pre_bline * fs))

    if not epoch_end < int(s_post * fs) + int(s_pre * fs):
        print('Epoch end index beyond trial duration. Check your slice indices.')
        sys.exit(1)

    R1_angles = mwt_angle(R1_trials, freqs, ns)
    R2_angles = mwt_angle(R2_trials, freqs, ns)

    R1_angles = [[f[baseline_end:epoch_end] for f in trial] for trial in R1_angles]
    R2_angles = [[f[baseline_end:epoch_end] for f in trial] for trial in R2_angles]

    ISPCs = calc_ISPC(R1_angles, R2_angles)

    return ISPCs

def plot_ISPCs(ispc, epoch_len, pre_bline, freqs, global_labels, local_labels):
    # note: pre_bline and epoch_len are in SECONDS.

    f, ax = plt.subplots(1,1)
    f.set_figheight(5)
    f.set_figwidth(7)
    title = ' '.join([global_labels, local_labels])

    fs = 2000
    freqs = freqs
    time = np.arange(-1*pre_bline, epoch_len/2, 1/fs)*1000   # *1000 to get into ms
    n_levels = 40
    levels = np.linspace(0, 1, n_levels+1)

    cs = ax.contourf(time, freqs, ispc, n_levels, levels=levels)
    ax.set_title(title)
    ax.set(xlabel='Time (ms), press @ t=0', ylabel='Frequency (Hz)')
    f.colorbar(cs, ax=ax, shrink=0.9)

    frange = f'{freqs.min()}-{freqs.max()} Hz'
    f.savefig(' '.join([title, frange, 'ISPC.png']), bbox_inches='tight', dpi=600)
    plt.close()

# Phase-Amplitude Coupling (not Z scored)
def butter_bandpass(lowcut, highcut, fs, order):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')

    return b, a

def butter_bandpass_filter(data, lowcut, highcut, fs, order):
    b, a = butter_bandpass(lowcut, highcut, fs, order)
    y = filtfilt(b, a, data)

    return y

def filter_hilbert(sig, lc, hc, fs, order):
    filt = butter_bandpass_filter(sig, lc, hc, fs, order)
    asig = hilbert(filt)

    return(asig)

def get_bin_indices(epoch):

    # bins of pi/50 are hard coded here.
    # for each pi/50 bin, generating an array of booleans where
    # phase value falls between bin_low and bin_high
    epoch_bin_bool = []
    for i in np.arange(-50,50):
        bin_low = (np.pi*i)/50
        bin_high = (np.pi*(i+1))/50
        bin1 = (bin_low < epoch) & (epoch < bin_high)

        epoch_bin_bool.append(bin1.tolist())

    # getting indices where True
    epoch_bin_indices = [np.argwhere(binned_epoch).tolist() for binned_epoch in epoch_bin_bool]

    # flattening the list of lists in our list of lists of lists... >:-(
    # lmao i didn't even need this but it's staying in here bc
    # this list (of lists) comprehension looks ridiculous
    epoch_bin_indices = [[index for lst in pha_bin for index in lst] for pha_bin in epoch_bin_indices]

    return(epoch_bin_bool)

def get_mwtpower_by_phasebins(trial_n_f_n, trial_n_bin, n_freqs, n_bins):
    '''
    modified version of get_power_by_phasebins from last notebook for use with
    outputs from neurodsps's compute_wavelet_transform.
    '''
    sample_i = []
    for i in range(0, n_freqs):

        sample_i_bin_j = []
        for j in range(0, n_bins):

            p = np.array(trial_n_f_n[i])[np.array(trial_n_bin[j])]
            sample_i_bin_j.append(p.mean())

        sample_i.append(sample_i_bin_j)

    return(sample_i)

# next just extract and normalize power as before using mwt
# then just index the power by the phase of the signal (extracted
# via filter-hilbert as described above)

def pow_by_pha(pow_series, pha_bins, nfs, nbins):
    powbypha = []
    for pow_trial, pha_bins in zip(pow_series, pha_bins):
        trial = get_mwtpower_by_phasebins(pow_trial, pha_bins, n_freqs=nfs, n_bins=nbins)
        powbypha.append(trial)
    return powbypha

# the following code won't work in this context but something along these lines:
def get_composite_signal(trials_pha, trials_pow, s_pre, s_post, epoch_len, pre_bline, freqs):
    # theta phase time series\
    fs = 2000
    epoch_samples = epoch_len * fs
    epoch_start = int((s_pre * fs ) - (epoch_samples / 2))          # slices symmetrically around timestamps
    epoch_end = int(epoch_samples) + epoch_start
    baseline_end = int((epoch_samples / 2) - (pre_bline * fs))      # baseline begins at the beginning of the trial.
                                                                    # and ends some number of ms right before timestamp
    if not epoch_end < int(s_post * fs) + int(s_pre * fs):
        print('Epoch end index beyond trial duration. Check your slice indices.')
        sys.exit(1)

    pha = [np.angle(filter_hilbert(sig=trial, lc=6, hc=10, fs=fs, order=4)) for trial in trials_pha]
    pha = [trial[epoch_start:epoch_end] for trial in pha]   # a bit unnecessary but slicing the same way we do with power
    pha = [trial[baseline_end:] for trial in pha]           # just doing it this way for consistency
    pha_bins = [get_bin_indices(trial) for trial in pha]

    # gamma power time series
    fs = 2000
    n_dict = dict([(i, 3+(i-1)*9/(len(range(20,121))-1)) for i in range(20,121)]) # scale n with freq
    freqs = freqs
    mwt = [[compute_mwt(trial, fs=fs, peak_freq=f, n=n_dict[f]) for f in freqs] for trial in trials_pow]
    pow = [(abs(np.array(epoch)))**2 for epoch in mwt]
    norm_pow, epoch_len, pre_bline = baseline_norm(pow, s_pre, s_post, epoch_len, pre_bline)

    return pha_bins, norm_pow

def doubleplot(powbypha):
    singleplot = powbypha
    tripleplot = np.concatenate([singleplot.T,singleplot.T,singleplot.T]).T
    doubleplot = tripleplot.T[50:250].T #[50:250] here because we want to start at 0 degrees, not -180.

    return doubleplot

def plot_PAC(powbypha, global_labels, local_labels, regions):
    f, ax = plt.subplots(1,1)
    f.set_figheight(5)
    f.set_figwidth(7)
    title = ' '.join([global_labels, local_labels])
    r_power = regions.upper().split('-')[0]
    r_phase = regions.upper().split('-')[1]

    freqs = np.arange(20,121)
    pha_bin = np.linspace(0,720,200)
    levels = np.linspace(powbypha.min(),powbypha.max()+0.1)       # play around with these then standardize for the study.

    cs = ax.contourf(pha_bin, freqs, powbypha, levels=levels, cmap='jet')
    ax.set_title(title)
    ax.set(xlabel=r_phase +' Theta Phase (Ëš)', ylabel=r_power +' Frequency (Hz)')
    f.colorbar(cs, ax=ax, shrink=0.9)

    f.savefig(' '.join([title, local_labels, 'PAC.png']), bbox_inches='tight', dpi=600)
    plt.close()

def calc_PLI(R1_angles, R2_angles):
    '''
    R1_angles, R2_angles are timeseries instantaneous phase angles from respective
    regions R1 and R2 for a list trials. Both are expected to be a 2D array
    organized first by f, then by sample (signals)

    returns a 2D array of phase lag indices between the two regions organized first
    by f, then by sample.
    '''
    R1_angles_T = np.transpose(R1_angles)
    R2_angles_T = np.transpose(R2_angles)

    # finding our angle differences
    R1_R2_angles_diff_T = R1_angles_T - R2_angles_T

    R1_R2_PLIs = []
    for sig in R1_R2_angles_diff_T:

        PLI_byfreq = []
        for freq in sig:
            # compute PLI
            PLI_byfreq.append(np.abs(np.mean(np.sign(np.imag(np.exp(1j * freq))))))

        R1_R2_PLIs.append(PLI_byfreq)

    R1_R2_PLIs = np.transpose(R1_R2_PLIs)

    return R1_R2_PLIs

def PLI(R1_trials, R2_trials, s_pre, s_post, pre_bline, epoch_len, freqs, ns):
    '''
    wrapped ISPC method with hard coded params for low freq ISPCs with 1 s time
    window. edit here for different params.
    '''
    #freqs = np.logspace(np.log10(4), np.log10(40),20)
    freqs=freqs

    fs = 2000
    epoch_samples = fs * epoch_len
    epoch_start = int((s_pre * fs) - (epoch_samples / 2))
    epoch_end = int(epoch_samples) + epoch_start
    baseline_end = int((s_pre * fs) - (pre_bline * fs))

    if not epoch_end < int(s_post * fs) + int(s_pre * fs):
        print('Epoch end index beyond trial duration. Check your slice indices.')
        sys.exit(1)

    R1_angles = mwt_angle(R1_trials, freqs, ns)
    R2_angles = mwt_angle(R2_trials, freqs, ns)

    R1_angles = [[f[baseline_end:epoch_end] for f in trial] for trial in R1_angles]
    R2_angles = [[f[baseline_end:epoch_end] for f in trial] for trial in R2_angles]

    PLIs = calc_PLI(R1_angles, R2_angles)

    return PLIs

def plot_PLIs(pli, epoch_len, pre_bline, freqs, global_labels, local_labels):
    # note: pre_bline and epoch_len are in SECONDS.

    f, ax = plt.subplots(1,1)
    f.set_figheight(5)
    f.set_figwidth(7)
    title = ' '.join([global_labels, local_labels])

    fs = 2000
    freqs = freqs
    time = np.arange(-1*pre_bline, epoch_len/2, 1/fs)*1000   # *1000 to get into ms
    n_levels = 40
    levels = np.linspace(0, 1, n_levels+1)

    cs = ax.contourf(time, freqs, pli, n_levels, levels=levels)
    ax.set_title(title)
    ax.set(xlabel='Time (ms), press @ t=0', ylabel='Frequency (Hz)')
    f.colorbar(cs, ax=ax, shrink=0.9)

    frange = f'{freqs.min()}-{freqs.max()} Hz'
    f.savefig(' '.join([title, frange, 'PLI.png']), bbox_inches='tight', dpi=600)
    plt.close()

# while we've got our PLIs, we might as well also calculate a directed PLI
# to give us some insight onto the direction of information flow b/w two regions
def calc_dPLI(R1_angles, R2_angles):
    '''
    calculates the directed phase lag index by applying heaviside step function to
    the difference of instantaneous phase angles between two regions, that is,
    R1_angles - R2_angles. A positive dPLI score indicates that on average,
    R1 tends to lead R2 (for that frequency).
    '''

    R1_angles_T = np.transpose(R1_angles)
    R2_angles_T = np.transpose(R2_angles)

    # finding our angle differences
    R1_R2_angles_diff_T = R1_angles_T - R2_angles_T

    R1_R2_dPLIs = []
    for sig in R1_R2_angles_diff_T:

        dPLI_byfreq = []
        for freq in sig:
            # compute PLI
            dPLI_byfreq.append(np.mean(np.heaviside(freq, 0.5)))

        R1_R2_dPLIs.append(dPLI_byfreq)

    R1_R2_dPLIs = np.transpose(R1_R2_dPLIs)

    return R1_R2_dPLIs

def dPLI(R1_trials, R2_trials, s_pre, s_post, pre_bline, epoch_len, freqs, ns):
    '''
    wrapped directed phase lag method with hard coded params for low freq with 1 s time
    window. edit here for different params.
    '''
    #freqs = np.logspace(np.lot10(4), np.log10(40),20)
    freqs=freqs

    fs = 2000
    epoch_samples = fs * epoch_len
    epoch_start = int((s_pre * fs) - (epoch_samples / 2))
    epoch_end = int(epoch_samples) + epoch_start
    baseline_end = int((s_pre * fs) - (pre_bline * fs))

    if not epoch_end < int(s_post * fs) + int(s_pre * fs):
        print('Epoch end index beyond trial duration. Check your slice indices.')
        sys.exit(1)

    R1_angles = mwt_angle(R1_trials, freqs, ns)
    R2_angles = mwt_angle(R2_trials, freqs, ns)

    R1_angles = [[f[baseline_end:epoch_end] for f in trial] for trial in R1_angles]
    R2_angles = [[f[baseline_end:epoch_end] for f in trial] for trial in R2_angles]

    dPLIs = calc_dPLI(R1_angles, R2_angles)

    return dPLIs

def plot_dPLIs(dpli, epoch_len, pre_bline, freqs, global_labels, local_labels):
    f, ax = plt.subplots(1,1)
    f.set_figheight(5)
    f.set_figwidth(7)
    title = ' '. join([global_labels, local_labels])

    fs = 2000
    freqs = freqs
    time = np.arange(-1*pre_bline, epoch_len/2, 1/fs)*1000   # *1000 to get into ms
    n_levels = 40
    levels = np.linspace(0, 1, n_levels+1)

    cs = ax.contourf(time, freqs, dpli, n_levels, levels=levels, cmap='bwr')
    ax.set_title(title)
    ax.set(xlabel='Time (ms), press @ t=0', ylabel='Frequency (Hz)')
    f.colorbar(cs, ax=ax, shrink=0.9)

    frange = f'{freqs.min()}-{freqs.max()} Hz'
    f.savefig(' '.join([title, frange, 'dPLI.png']), bbox_inches='tight', dpi=600)
    plt.close()

def norm_pow(trials, freqs, n_dict, s_pre, s_post, epoch_len, pre_bline):
    '''
    wrapped spectrogram function for high frequncy analysis.
    '''
    if not trials is None:
        fs = 2000
        freqs = freqs

        mwt = [[compute_mwt(trial, fs=fs, peak_freq=f, n=n_dict[f]) for f in freqs] for trial in trials]
        pow = [(abs(np.array(epoch)))**2 for epoch in mwt]
        nrm_pow, epoch_len, pre_bline = baseline_norm(pow, s_pre, s_post, epoch_len, pre_bline)

        return nrm_pow

    else:
        return None

def corrolate_power(R1_trial, R2_trial):
    R1_T = np.transpose(R1_trial)
    R2_T = np.transpose(R2_trial)

    s_r_T = []
    for i in range(len(R1_T)):

        sample_R1 = R1_T[i]
        sample_R2 = R2_T[i]

        fs = []
        for j in range(len(sample_R1)):
            f_R1 = sample_R1[j]
            f_R2 = sample_R2[j]

            #print(list(zip(f_R1, f_R2)))

            r, p = stats.spearmanr(f_R1, f_R2)

            fs.append(r)

        spearmans_r_T.append(fs)

    corr_coefficients = np.transpose(spearmans_r_T)

    return corr_coefficients


def plot_corrpow(cpow, epoch_len, pre_bline, freqs, global_labels, local_labels):
    f, ax = plt.subplots(1,1)
    f.set_figheight(5)
    f.set_figwidth(7)
    title = ' '. join([global_labels, local_labels])

    fs = 2000
    freqs = freqs
    time = np.arange(-1*pre_bline, epoch_len/2, 1/fs)*1000   # *1000 to get into ms
    n_levels = 40
    levels = np.linspace(-1, 1, n_levels+1)

    cs = ax.contourf(time, freqs, cpow, n_levels, levels=levels, cmap='jet')
    ax.set_title(title)
    ax.set(xlabel='Time (ms), press @ t=0', ylabel='Frequency (Hz)')
    f.colorbar(cs, ax=ax, shrink=0.9)

    frange = f'{freqs.min()}-{freqs.max()} Hz'
    f.savefig(' '.join([title, frange, 'Corrolated Power (txt, fxf; cbar=rho).png']), bbox_inches='tight', dpi=600)
    plt.close()

def bp_filter_sigs(sigs, lc, hc, fs, order, s_pre, s_post, epoch_len, pre_bline):
    filtered_sigs = [butter_bandpass_filter(sig, lc, hc, fs, order) for sig in sigs]

    epoch_samples = fs * epoch_len
    epoch_start = int((s_pre * fs) - (epoch_samples / 2))
    epoch_end = int(epoch_samples) + epoch_start
    baseline_end = int((s_pre * fs) - (pre_bline * fs))


    return [sig[baseline_end:epoch_end] for sig in filtered_sigs]

def plot_filtered_sigs(bp_sigs, lc, hc, epoch_len, pre_bline, global_labels, local_labels):
    # plotting filtered sigs (just bp filtered) to check that spectrograms are
    # reasonable/make sense.
    fs = 2000
    time = np.arange(-1*pre_bline, epoch_len/2, 1/fs)*1000     # *1000 to get into ms
    title = ' '.join([global_labels, local_labels])

    for i, bp_sig in enumerate(bp_sigs):
        f, ax = plt.subplots(1,1)
        f.set_figheight(5)
        f.set_figwidth(7)

        ax.set_title(' '.join([title, f'trial_{i}']))
        ax.plot(time, bp_sig)

        f.savefig(' '.join([title, f'trial_{i}', f'{lc}-{hc} Hz bp filtered trace.png']))
        plt.close()

def plot_filtered_sigs_2R(R1_bp_sigs, R2_bp_sigs, lc, hc, epoch_len, pre_bline, global_labels, local_labels):
    '''
    plot raw sigs from 2 regions (for power power correlations) to check that
    signals are as similar as the correlations suggest.
    raw signals here because the ABC animals indicated a high level of correlation
    across all f
    '''

    fs = 2000
    time = np.arange(-1*pre_bline, epoch_len/2, 1/fs)*1000     # *1000 to get into ms
    title = ' '.join([global_labels, local_labels])

    for i, (R1_bp_sig, R2_bp_sig) in enumerate(zip(R1_bp_sigs, R2_bp_sigs)):
        f, ax = plt.subplots(1,1)
        f.set_figheight(5)
        f.set_figwidth(7)

        ax.set_title(' '.join([title, f'trial_{i}']))
        ax.plot(time, R1_bp_sig)
        ax.plot(time, R2_bp_sig)

        f.savefig(' '.join([title, f'trial_{i}', f'{lc}-{hc} Hz bp filtered trace.png']))
        plt.close()

######## quantify ########

def normpow_timefreq_window(norm_pow_trials, freqs, f_range, epoch_len, pre_bline, t_range):
    '''
    each trial in norm_pow is organized first by f, then by time: a 2d array
    norm_pow is a list of 2d trials: a 3d array.

    freqs = something like np.arange(1,121)
    epoch_len, pre_bline are GIVEN IN SECONDS
        epoch_len: the duration of epoch before baselining, event timestamp is exactly halfway
        pre_bline: the duration to plot before event timestamp (the end of the baseline period)

    f_range is a tuple which defines the beginning and end frequencies to slice
    t_range is a tuple which defines the beginning and end time to slice, GIVEN
        IN MILLISECONDS RELATIVE TO EVENT TIMESTAMP

    '''
    # building a dictionary. i mean i could also use map here but this seemed
    # more straight forward to me at the time.

    d_f_indx = dict([(f, i) for i, f in enumerate(freqs)])

    fs = 2000           # ASSUME TIME IS GIVEN IN MS
    time_ms = np.round(np.arange(-pre_bline, epoch_len/2, 1/fs)*1000, 1)
    d_t_indx = dict([(t, i) for i, t in enumerate(time_ms)])

    # unpacking vars:
    f_i, f_j = f_range
    t_i, t_j = t_range

    tf_win_trials = []
    for trial in norm_pow_trials:
        f_win = trial[d_f_indx[f_i]:d_f_indx[f_j]]
        tf_win = [f[d_t_indx[t_i]:d_t_indx[t_j]] for f in f_win]

    return tf_win_trials

def tf_window(trial, f_range, t_range, freqs, epoch_len, pre_bline):
    d_f_indx = dict([(f, i) for i, f in enumerate(freqs)])

    fs = 2000           # ASSUME TIME IS GIVEN IN MS
    time_ms = np.round(np.arange(-pre_bline, epoch_len/2, 1/fs)*1000, 1)
    d_t_indx = dict([(t, i) for i, t in enumerate(time_ms)])

    # unpacking vars:
    f_i, f_j = f_range
    t_i, t_j = t_range

    tf_win_trials = []

    f_win = trial[d_f_indx[f_i]:d_f_indx[f_j]]
    tf_win = [f[d_t_indx[t_i]:d_t_indx[t_j]] for f in f_win]

    return tf_win

def windowed_spect_averages(trials_by_rat, f_range, t_range, freqs, n_dict, s_pre, s_post, epoch_len, pre_bline):
    windowed_spects = []
    for rat_trials in trials_by_rat:
        rat_spectrogram = spectrogram(rat_trials, freqs, n_dict, s_pre, s_post, epoch_len, pre_bline)
        windowed_spect = tf_window(rat_spectrogram, f_range, t_range, freqs, epoch_len, pre_bline)
        windowed_spects.append(windowed_spect)

    windowed_averages = [np.average(tf_win) for tf_win in windowed_spects]

    return windowed_spects, windowed_averages

def windowed_pow_averages(trials, f_range, t_range, freqs, n_dict, s_pre, s_post, epoch_len, pre_bline):
    windowed_power = []
    pow = norm_pow(trials, freqs, n_dict, s_pre, s_post, epoch_len, pre_bline)
    for trial_norm_pow in pow:
        windowed = tf_window(trial_norm_pow, f_range, t_range, freqs, epoch_len, pre_bline)
        windowed_power.append(windowed)


    avg_windowed_pow = [np.average(tf_win) for tf_win in windowed_power]

    for trial, pow in zip(trials, avg_windowed_pow):
        if pow > 10:
            plt.plot(trial)
            plt.show()

    return windowed_power, avg_windowed_pow

def armorf(x: np.ndarray, Nr = 1, Nl = 10, p = 2)->tuple:
    '''
    AR parameter estimation via LWR method by Morf modified
    args:
        x(L, N): np.ndarray, L: number of variables, N: number of samples(time points);
        Nr: int, number of realizations;
        Nl: int, length of every realization;
        p: int, order of the AR model;
    return:
        coeff: np.ndarray, AR coefficients;
        En: np.ndarray, final prediction error.
    Ref:M. Morf, etal, Recursive Multichannel Maximum Entropy Spectral Estimation,
                IEEE trans. GeoSci. Elec., 1978, Vol.GE-16, No.2, pp85-94.
        S. Haykin, Nonlinear Methods of Spectral Analysis, 2nd Ed.
            Springer-Verlag, 1983, Chapter 2
        Jie Cui, Lei Xu, Steven L. Bressler, Mingzhou Ding, Hualou Liang,
            BSMART: a Matlab/C toolbox for analysis of multichannel neural time series, Neural Networks, 21:1094 - 1104, 2008.
    '''
    if x.ndim == 1: x = x[np.newaxis, :]

    L, N = x.shape
    R0 = np.zeros((L, L))
    pf, pb, pfb, ap, bp, En = R0.copy(), R0.copy(), R0.copy(), R0.copy(), R0.copy(), R0.copy()

    for i in np.arange(1, Nr+1):
        En = En + x[:, (i-1)*Nl:i*Nl] @ x[:, (i-1)*Nl:i*Nl].T
        ap = ap + x[:, (i-1)*Nl + 1:i*Nl] @ x[:, (i-1)*Nl + 1:i*Nl].T
        bp = bp + x[:, (i-1)*Nl:i*Nl-1] @ x[:, (i-1)*Nl:i*Nl-1].T

    ap = inv(cholesky(ap* (Nl-1)))
    bp = inv(cholesky(bp* (Nl-1)))

    for i in np.arange(1, Nr+1):

        efp = ap @ x[:, (i-1)*Nl+1:i*Nl]
        ebp = bp @ x[:, (i-1)*Nl:i*Nl-1]
        pf = pf + efp @ efp.T
        pb = pb + ebp @ ebp.T
        pfb = pfb + efp @ ebp.T
    En = cholesky(En/N)

    # Initial output variables
    coeff = []
    kr=[]

    for m in np.arange(1, p+1):
        ck = inv(cholesky(pf)) @ pfb @ inv(cholesky(pb).T)
        kr.append(ck)

        # Update the forward and backward prediction errors
        ef = np.eye(L) - ck @ ck.T
        eb = np.eye(L) - ck.T @ ck

        # Update the prediction error
        En = En @ cholesky(ef)
        E = (ef +eb)/2

        # Update the coefficients of the forward and backward prediction errors
        ap = np.dstack((ap, np.zeros((L, L))))
        bp = np.dstack((bp, np.zeros((L, L))))

        pf = np.zeros((L, L))
        pb = np.zeros((L, L))
        pfb = np.zeros((L, L))

        a = np.zeros((L, L, m + 1))
        b = np.zeros((L, L, m + 1))
        for i in np.arange(1, m+2):
            a[:, :, i-1] = inv(cholesky(ef)) @ (ap[:,:,i-1] - ck @ bp[:, :, m+1-i])
            b[:, :, i-1] = inv(cholesky(eb)) @ (bp[:,:,i-1] - ck.T @ ap[:, :, m+1-i])
        for k in np.arange(1, Nr+1):
            efp = np.zeros((L, Nl - m - 1))
            ebp = np.zeros((L, Nl - m - 1))

            for i in np.arange(1, m+2):
                k1 = m+2-i+(k-1)*Nl+1
                k2 = Nl-i+1+(k-1)*Nl

                efp = efp + a[:, :, i-1] @ x[:, k1-1:k2]
                ebp = ebp + b[:, :, m+1-i] @ x[:, k1-2:k2-1]

            pf = pf + efp @ efp.T
            pb = pb + ebp @ ebp.T
            pfb = pfb + efp @ ebp.T

        ap = a
        bp = b

    for j in np.arange(1, p+1):
        coeff.append(inv(a[:, :, 0]) @ a[:, :, j])

    return -np.asarray(coeff), En @ En.T

def granger_preprocessing(trial):
    # detrend and z normalize
    trial = stats.zscore(signal.detrend(trial))

    return trial

def granger(R1, R2, t_win, order, _bic=False):
    '''
    computes granger causality estimate over time between R1 and R2
    '''
    assert np.shape(R1) == np.shape(R2)
    q =  4
    R1 = [decimate(trial, q=q) for trial in R1]
    R2 = [decimate(trial, q=q) for trial in R2]

    fs = int(2000/q)

    timewin = t_win #ms
    timewin_points = int((timewin/1000)*fs)

    trial_ns = len(R1)
    trial_len = len(R1[0])

    bic = []
    GC_E_y2x = []
    GC_E_x2y = []
    Ex_t = []
    Ey_t = []
    E_00 = []
    E_11 = []

    for i in range(trial_len - timewin_points):
        tempdata = np.array([
            [granger_preprocessing(trial[i:i+timewin_points]) for trial in R1],
            [granger_preprocessing(trial[i:i+timewin_points]) for trial in R2]
        ])

        Ax, Ex = armorf(np.concatenate(tempdata[0,:]), trial_ns, timewin_points, order)
        Ay, Ey = armorf(np.concatenate(tempdata[1,:]), trial_ns, timewin_points, order)
        Axy, E = armorf(
            np.array([np.concatenate(tempdata[0,:]), np.concatenate(tempdata[1,:])]),
            trial_ns, timewin_points, order
        )

        GC_E_y2x.append(np.log(Ex/E[0,0]))
        GC_E_x2y.append(np.log(Ey/E[1,1]))

        Ex_t.append(Ex)
        Ey_t.append(Ey)
        E_00.append(E[0,0])
        E_11.append(E[1,1])

        if _bic == True:

            bic_t = []
            for bici in np.arange(1,16):
                Axy, E = armorf(
                    np.array([np.concatenate(tempdata[0,:]), np.concatenate(tempdata[1,:])]),
                    trial_ns, timewin_points, bici
                )
                b = np.log(np.linalg.det(E)) + ((np.log(timewin_points)*bici*2**2)/timewin_points)

                bic_t.append(b)

            bic.append(bic_t)

    if _bic == False:
        return GC_E_y2x, GC_E_x2y, Ex_t, Ey_t, E_00, E_11

    elif _bic == True:
        return np.array(bic).T

### granger by trial ###
def granger_bytrial(R1, R2, t_win, order, _bic=False):
    '''
    computes granger causality estimate over time between R1 and R2
    '''
    assert np.shape(R1) == np.shape(R2)
    q=4
    R1 = [decimate(trial, q=q) for trial in R1]
    R2 = [decimate(trial, q=q) for trial in R2]
    fs = int(2000/q)

    timewin = t_win #ms
    timewin_points = int((timewin/1000)*fs)

    trial_ns = len(R1)
    trial_len = len(R1[0])

    bic = []
    GC_y2x = []
    GC_x2y = []
    #Ex_t = []
    #Ey_t = []
    #E_00 = []
    #E_11 = []

    for i in range(trial_len - timewin_points):

        GC_y2x_bytrial = []
        GC_x2y_bytrial = []

        for R1_trial, R2_trial in zip(R1, R2):
            tempdata = np.array([
            [granger_preprocessing(R1_trial[i:i+timewin_points])],
            [granger_preprocessing(R2_trial[i:i+timewin_points])]
            ])

            Ax, Ex = armorf(np.concatenate(tempdata[0,:]), 1, timewin_points, order)
            Ay, Ey = armorf(np.concatenate(tempdata[1,:]), 1, timewin_points, order)
            Axy, E = armorf(
                np.array([np.concatenate(tempdata[0,:]), np.concatenate(tempdata[1,:])]),
                1, timewin_points, order
            )

            GC_y2x_bytrial.append(np.log(Ex/E[0,0]))
            GC_x2y_bytrial.append(np.log(Ey/E[1,1]))

        GC_y2x.append(GC_y2x_bytrial)
        GC_x2y.append(GC_x2y_bytrial)

        #Ex_t.append(Ex)
        #Ey_t.append(Ey)
        #E_00.append(E[0,0])
        #E_11.append(E[1,1])


        if _bic == True:

            bic_t = []
            for bici in np.arange(1,16):
                Axy, E = armorf(
                    np.array([np.concatenate(tempdata[0,:]), np.concatenate(tempdata[1,:])]),
                    trial_ns, timewin_points, bici
                )
                b = np.log(np.linalg.det(E)) + (np.log(timewin_points)*bici*2**2)/timewin_points

                bic_t.append(b)

            bic.append(bic_t)

    if _bic == False:
        return GC_y2x, GC_x2y

    elif _bic == True:
        return np.array(bic).T

def get_null_GCe(R1, R2, t_win, order):
    '''
    computes granger causality estimate over time between R1 and R2
    '''
    assert np.shape(R1) == np.shape(R2)

    R1 = get_timeshifted_null(R1)

    R1 = [decimate(trial, q=4) for trial in R1]
    R2 = [decimate(trial, q=4) for trial in R2]
    fs = 500

    timewin = t_win #ms
    timewin_points = int((timewin/1000)*fs)

    trial_ns, trial_len = np.shape(R1)[0], np.shape(R1)[1]

    GC_E_y2x = []
    GC_E_x2y = []

    for i in range(trial_len - timewin_points):
        tempdata = np.array([
            [granger_preprocessing(trial[i:i+timewin_points]) for trial in R1],
            [granger_preprocessing(trial[i:i+timewin_points]) for trial in R2]
        ])

        Ax, Ex = armorf(np.concatenate(tempdata[0,:]), trial_ns, timewin_points, order)
        Ay, Ey = armorf(np.concatenate(tempdata[1,:]), trial_ns, timewin_points, order)
        Axy, E = armorf(
            np.array([np.concatenate(tempdata[0,:]), np.concatenate(tempdata[1,:])]),
            trial_ns, timewin_points, order
        )

        GC_E_y2x.append(np.squeeze(np.log(Ex/E[0,0])))
        GC_E_x2y.append(np.squeeze(np.log(Ey/E[1,1])))

    return GC_E_y2x, GC_E_x2y

def get_zscored_GCe(R1, R2, t_win, order, n_perm):
    # start = datetime.datetime.now()
    # null_GCe = [get_null_GCe(R1, R2, t_win, order) for i in range(n_perm)]
    # null_GCe_y2x = np.array([y2x for y2x, x2y in null_GCe]).T     # transpose to quickly access null distributions across each sample
    # null_GCe_x2y = np.array([x2y for y2x, x2y in null_GCe]).T
    # end = datetime.datetime.now()

    # print(end-start)

    # sys.exit(0)

    null_GCe_y2x = []
    null_GCe_x2y = []
    for i in range(n_perm):
        start = datetime.datetime.now()

        y2x, x2y = get_null_GCe(R1, R2, t_win, order)

        end = datetime.datetime.now()

        print(f'permutation {i} duration: {end-start}')

        null_GCe_y2x.append(y2x)
        null_GCe_x2y.append(x2y)

    null_GCe_y2x = np.array(null_GCe_y2x).T
    null_GCe_x2y = np.array(null_GCe_x2y).T


    observed_GCe_y2x, observed_GCe_x2y, Ex_t, Ey_t, E_00, E_11 = granger(R1, R2, t_win, order, _bic=False)

    observed_GCe_y2x = np.squeeze(observed_GCe_y2x)
    observed_GCe_x2y = np.squeeze(observed_GCe_x2y)

    zscored_GCe_y2x = [(observed_GCe_y2x[i] - np.average(null_distribution)) / np.std(null_distribution) for i, null_distribution in enumerate(null_GCe_y2x)]
    zscored_GCe_x2y = [(observed_GCe_x2y[i] - np.average(null_distribution)) / np.std(null_distribution) for i, null_distribution in enumerate(null_GCe_x2y)]

    return zscored_GCe_y2x, zscored_GCe_x2y




def get_composite_signal_PACz(trials_pha, trials_pow, freq4pha, freq4pow, s_pre, s_post, epoch_len, pre_bline, t_start, t_end):
    # theta phase time series\
    def _baseline_norm(Tr_pow, s_pre, s_post, epoch_len, pre_bline):
        # calculate epoch and baseline indices from s_pre, s_post. I hate always doing
        # this by hand so let's just write it in terms of s_pre, s_post which are given
        '''
        short fn to slice baseline and epoch from power extracted from mwt.
        For peak f (defined by params of convolution) takes the average value in
        baseline slice and divides every value in epoch slice by that value.
        '''
        fs = 2000
        epoch_samples = epoch_len * fs                              # the duration of our entire epoch we want to cut out
        epoch_start = int((s_pre * fs ) - (epoch_samples / 2))      # the sample (index) where our epoch begins
        epoch_end = int(epoch_samples) + epoch_start                # the sample (index) where our epoch ends (the start i + total epoch duration)
        baseline_end = int((epoch_samples / 2) - (pre_bline * fs))  # the sample, i where [0:i] will be used for baselining

        if not epoch_end < int(s_post * fs) + int(s_pre * fs):
            print('Epoch end index beyond trial duration. Check your slice indices.')
            sys.exit(1)

        # cutting off our "wings"
        Trpow = [epoch[epoch_start:epoch_end] for epoch in Tr_pow]

        # defining our baseline epochs
        Trbase = [epoch[0:baseline_end]for epoch in Trpow]

        # defining our signal epochs
        Trsig = [epoch[baseline_end:] for epoch in Trpow]

        # means of baselines
        Trbase_fmean = [np.average(epoch) for epoch in Trbase]

        # normlize to baseline
        Tr_norm = []
        for i in range(len(Trsig)):
            # normalized = np.array(Trsig[i]) / Trbase_fmean[i]
            normalized = np.array(Trsig[i])
            Tr_norm.append(normalized)

        return Tr_norm, epoch_len, pre_bline    # return epoch_len, pre_bline for plotting.

    fs = 2000
    epoch_samples = epoch_len * fs
    epoch_start = int((s_pre * fs ) - (epoch_samples / 2))          # slices symmetrically around timestamps
    epoch_end = int(epoch_samples) + epoch_start
    baseline_end = int((epoch_samples / 2) - (pre_bline * fs))      # baseline begins at the beginning of the trial.
                                                                    # and ends some number of ms right before timestamp
    if not epoch_end < int(s_post * fs) + int(s_pre * fs):
        print('Epoch end index beyond trial duration. Check your slice indices.')
        sys.exit(1)

    #pha = [np.angle(filter_hilbert(sig=trial, lc=freq4pha-1, hc=freq4pha+1, fs=2000, order=4)) for trial in trials_pha]

    n_dict =  dict([(i, 3+(i-1)*3/(len(np.arange(1,41)-1))) for i in np.arange(1,41,0.5)])
    mwt = [compute_mwt(trial, fs=fs, peak_freq=freq4pha, n=n_dict[freq4pha]) for trial in trials_pow]
    pha = [np.angle(trial) for trial in mwt]
    pha = [trial[epoch_start:epoch_end] for trial in pha]   # a bit unnecessary but slicing the same way we do with power
    pha = [trial[baseline_end:] for trial in pha]           # just doing it this way for consistency
    pha = [trial[int(pre_bline * fs):] for trial in pha]    # cut out baseline (len = 1/2 epoch_samples, from t=0:)
    pha = [trial[int(t_start * fs):int(t_end * fs)] for trial in pha] # final cut to only analyze specific t_win
    pha_bins = [get_bin_indices(trial) for trial in pha]

    # gamma power time series
    fs = 2000
    n_dict = dict([(i, 3+(i-1)*9/(len(range(20,121))-1)) for i in range(20,121)]) # scale n with freq
    mwt = [compute_mwt(trial, fs=fs, peak_freq=freq4pow, n=n_dict[freq4pow]) for trial in trials_pow]
    pow = [(abs(np.array(epoch)))**2 for epoch in mwt]
    norm_pow, epoch_len, pre_bline = _baseline_norm(pow, s_pre, s_post, epoch_len, pre_bline)
    pwr = [trial[int(pre_bline * fs):] for trial in norm_pow]   # cut out baseline (len = 1/2 epoch_samples, from t=0:)
    pwr = [trial[int(t_start * fs):int(t_end * fs)] for trial in pwr]   # final cut to only analyze specific t_win

    # concat for z scoring
    pha = np.concatenate(pha)
    pwr = np.concatenate(pwr)

    return pha, pwr

def get_PAC(pha, pwr):
    PAC = abs(np.mean(pwr * np.exp(1j * pha)))

    return PAC

def get_dims(R1, R2, t_start, t_end):
    assert np.shape(R1) == np.shape(R2)

    fs = 2000
    d1 = len(R1)
    d2 = int((t_end * fs) - (t_start * fs))
    dims = (d1, d2)

    return dims

def get_PACz(pha, pwr, PAC, n_permutations, trial_dims):
    # trial_dims is tuple

    permutedPAC = []
    for i in range(0, n_permutations):

        pwr_trials = np.reshape(pwr, trial_dims)

        t_shifted_trials = []
        for trial in pwr_trials:
            rand_t = np.random.randint(len(trial))
            t_shifted_trial = np.concatenate([trial[rand_t:len(trial)],trial[0:rand_t]])

            t_shifted_trials.append(t_shifted_trial)

        permuted_pwr = np.concatenate(np.array(t_shifted_trials))

        PACsurr = abs(np.mean(permuted_pwr * np.exp(1j * pha)))
        permutedPAC.append(PACsurr)

    # normalize
    PACz = (PAC - np.mean(permutedPAC)) / np.std(permutedPAC)

    return(PACz)

def PACz_wrapped(trials_pha, trials_pow, freqs4phase, freqs4power, s_pre, s_post, epoch_len, pre_bline, t_start, t_end):
    '''
    takes two signals, filters and extracts theta phase and gamma power information and then
    calculates z scored PAC value between the two signals.
    '''
    PACz_byphasef = []
    for i in range(len(freqs4phase)):
        freq4pha = freqs4phase[i]

        PACz_bypowerf = []
        for j in range(len(freqs4power)):
            dims = get_dims(trials_pha, trials_pow, t_start, t_end)
            freq4pow = freqs4power[j]
            pha, pwr = get_composite_signal_PACz(trials_pha, trials_pow, freq4pha, freq4pow, s_pre, s_post, epoch_len, pre_bline, t_start, t_end)

            PACraw = get_PAC(pha=pha, pwr=pwr)
            PACz = get_PACz(pha=pha, pwr=pwr, PAC=PACraw, n_permutations=1000, trial_dims=dims)

            PACz_bypowerf.append(PACz)

        PACz_byphasef.append(PACz_bypowerf)
        print(freq4pha)

    return np.array(PACz_byphasef).T
        ##### from old green/white analyses #####





# def get_timeshifted_null(x_trials):
#     '''
#     x is a list of lists. Each list in x is a trial of len = to samples, the values
#     of in each trial are any kind of timeseries data, i.e. instantaneous pwr/pha etc.
#     '''
#     t_shifted_trials = []
#     for trial in pwr_trials:
#         rand_t = random.randrange(len(trial))
#         t_shifted_trial = np.concatenate([trial[rand_t:],trial[:rand_t]])
#
#         t_shifted_trials.append(t_shifted_trial)
#
#     return t_shifted_trials

def get_timeshifted_null(x_trials):
    '''
    x_trials is a list of trials where each trial is a list of timeseries data
    expect shape to be two dimensional

    takes trials from x_trials and randomly shifts them so that the trial starts at a random
    index ranging from 1:len(trial). trials are wrapped so that remaining trial data is concatenated
    to the end of the timeshifted trial.

    returns a list of time shifted trials.
    '''
    # draw n random integers from a uniform distribution ranging from 1 to l
    # where n is the number of trials and l is the length of each trial
    samp = np.random.uniform(1,np.shape(x_trials)[1], np.shape(x_trials)[0]).astype(int)

    # each trial is paired up with its own index from the random sample
    # trials are rearranged to begin at the random index and then concatenated
    # with remaining data at the end (wrapped)
    t_shifted_trials = [np.concatenate([trial[rand_t:],trial[:rand_t]]) for trial, rand_t in zip(x_trials, samp)]

    return t_shifted_trials


def get_zscored_PLI(x_trials, y_trials, n_permutations, observed_PLI, s_pre, s_post, pre_bline, epoch_len, freqs, ns):
    '''

    '''
    permuted_PLI = []
    for i in range(0, n_permutations):

        start = datetime.datetime.now()

        t_shifted_x_trials = get_timeshifted_null(x_trials)
        surrPLI = PLI(t_shifted_x_trials, y_trials, s_pre, s_post, pre_bline, epoch_len, freqs, ns)
        permuted_PLI.append(surrPLI)


        end = datetime.datetime.now()       # permutations. Let's just see the first 100
        print(f'permutation {i} duration: {end - start}')

    # I expect permuted_PLI to be a 3D array where each 2D array is a time frequency plot
    # of PLI calcualted between one permutation of our time shifted x series and y series.
    # we can then build a null distribution for each pixel in time frequency space
    # and compare each pixel in our observed PLI to it's respective null distribution.

    observed_PLI = np.array(observed_PLI)

    def _get_permuted_mean_std(permuted_x):
        '''
        I expected permuted_x to be a 3D array of  of len = n_permutations.
        each 2D array in permuted_PLI is a surrogate PLI in time frequency space
        '''
        f_avg_across_t = []
        f_std_across_t = []

        for t in np.array(permuted_x).T:
            f_avg_at_t = []
            f_std_at_t = []

            for f in t:
                avg_f = np.average(f)
                std_f = np.std(f, ddof=0)

                f_std_at_t.append(std_f)
                f_avg_at_t.append(avg_f)

            f_avg_across_t.append(f_avg_at_t)
            f_std_across_t.append(f_std_at_t)

        permuted_avgs = np.array(f_avg_across_t).T
        permuted_stds = np.array(f_std_across_t).T

        return permuted_avgs, permuted_stds

    permuted_avg, permuted_std = _get_permuted_mean_std(permuted_PLI)

    # the following calculation is only valid if arrays are the same shape and type
    # should be fine but let's just check.
    assert np.shape(permuted_avg) == np.shape(permuted_std)
    assert np.shape(permuted_avg) == np.shape(observed_PLI)

    assert type(permuted_avg) == type(permuted_std)
    assert type(permuted_avg) == type(observed_PLI)

    PLIz = (observed_PLI -  permuted_avg) / permuted_std

    return(PLIz)





def ITPC(trials, s_pre, s_post, pre_bline, epoch_len, freqs): #epoch_len = 2
    '''
    wrapped ITCP method with hard coded params for low freq ITPCs with 1 s time
    window. edit here for different params.
    expect freqs = np.arange(4,40)
    '''
    #freqs = np.logspace(np.lot10(4), np.log10(40),20)
    freqs=freqs
    ns = [6 + (i-4)*4/len(range(4,39)) for i in range(4,40)]    # edit here if freqs changes

    fs = 2000
    epoch_samples = fs * epoch_len
    epoch_start = int((s_pre * fs) - (epoch_samples / 2))
    epoch_end = int(epoch_samples) + epoch_start
    baseline_end = int((s_pre * fs) - (pre_bline * fs))

    if not epoch_end < int(s_post * fs) + int(s_pre * fs):
        print('Epoch end index beyond trial duration. Check your slice indices.')
        sys.exit(1)

    angles = mwt_angle(trials, freqs, ns)
    #print(f'shape of mwt_angle: {np.shape(angles)}')

    angles = [[f[epoch_start:epoch_end] for f in trial] for trial in angles]
    #print(f'shape after slicing: {np.shape(angles)}')

    ITPCs = calc_ITPC(angles)

    ITPC_baseline_subtraction = []
    for itpc in ITPCs:
        baseline_end_i = int(baseline_end - epoch_start)
        baseline_avg = np.average(itpc[0: baseline_end_i])

        itpc_baseline_subtraction = np.array(itpc) - baseline_avg
        ITPC_baseline_subtraction.append(itpc_baseline_subtraction[baseline_end_i:])

    #print(f'shape of calc_ITPC: {np.shape(ITPCs)}')

    return ITPC_baseline_subtraction


def get_zscored_ITPC(x_trials, n_permutations, observed_ITPC, s_pre, s_post, pre_bline, epoch_len, freqs):
    '''

    '''
    permuted_ITPC = []
    for i in range(0, n_permutations):
        if i < 100:
            start = datetime.datetime.now()

        t_shifted_x_trials = get_timeshifted_null(x_trials)
        surrITPC = ITPC(t_shifted_x_trials, s_pre, s_post, pre_bline, epoch_len, freqs)
        permuted_ITPC.append(surrITPC)

        if i < 100:                             # I don't want to see the timing of all 10000
            end = datetime.datetime.now()       # permutations. Let's just see the first 100
            print(f'permutation {i} duration: {end - start}')

    # I expect permuted_ITPC to be a 3D array where each 2D array is a time frequency plot
    # of ITPC calcualted between one permutation of our time shifted x series and y series.
    # we can then build a null distribution for each pixel in time frequency space
    # and compare each pixel in our observed ITPC to it's respective null distribution.

    observed_ITPC = np.array(observed_ITPC)

    def _get_permuted_mean_std(permuted_x):
        '''
        I expected permuted_x to be a 3D array of  of len = n_permutations.
        each 2D array in permuted_ITPC is a surrogate ITPC in time frequency space
        '''
        f_avg_across_t = []
        f_std_across_t = []

        for t in np.array(permuted_x).T:
            f_avg_at_t = []
            f_std_at_t = []

            for f in t:
                avg_f = np.average(f)
                std_f = np.std(f, ddof=0)

                f_std_at_t.append(std_f)
                f_avg_at_t.append(avg_f)

            f_avg_across_t.append(f_avg_at_t)
            f_std_across_t.append(f_std_at_t)

        permuted_avgs = np.array(f_avg_across_t).T
        permuted_stds = np.array(f_std_across_t).T

        return permuted_avgs, permuted_stds

    permuted_avg, permuted_std = _get_permuted_mean_std(permuted_ITPC)

    # the following calculation is only valid if arrays are the same shape and type
    # should be fine but let's just check.
    assert np.shape(permuted_avg) == np.shape(permuted_std)
    assert np.shape(permuted_avg) == np.shape(observed_ITPC)

    assert type(permuted_avg) == type(permuted_std)
    assert type(permuted_avg) == type(observed_ITPC)

    ITPCz = (observed_ITPC -  permuted_avg) / permuted_std

    return(ITPCz)


def PACz_cm(R1_trials, R2_trials, R1R2_label, n_perm, epoch_len, pre_bline, fs4pha, fs4pow, t_start, t_end, global_labels, s_pre):
    fs = 2000
    fname = ' '.join([global_labels, R1R2_label, 'z-scored PAC', f'{int(t_start*1000)}-{int(t_end*1000)}ms.npy'])
    twin_start = int((0.5*epoch_len * fs) + (t_start * fs))     # seconds after press
    twin_end = int((0.5*epoch_len * fs) + (t_end * fs))         # seconds after press

    print(np.shape(R1_trials))
    print(np.shape(R2_trials))

    assert len(R1_trials) == len(R2_trials)
    trial_n = len(R1_trials)

    print('\ncomputing z-scored PAC comodulogram...')



    fs = 2000
    epoch_samples = epoch_len * fs
    epoch_start = int((s_pre * fs ) - (epoch_samples / 2))          # slices symmetrically around timestamps
    epoch_end = int(epoch_samples) + epoch_start
    baseline_end = int((epoch_samples / 2) - (pre_bline * fs))

    def get_PAC(pha, pwr):
        PAC = abs(np.mean(pwr * np.exp(1j * pha)))
        return(PAC)

    pacz_by_phase = []
    for f_pha in fs4pha:

        pacz_by_power = []
        for f_pow in fs4pow:

            start = datetime.datetime.now()

            power = [abs(compute_mwt(trial, fs=2000, peak_freq=f_pow, n=12)) ** 2 for trial in R1_trials]     # high n for high f precision
            power = [trial[epoch_start:epoch_end] for trial in power]
            norm_power = [np.array(trial) / np.average(trial[:baseline_end]) for trial in power]
            norm_power_cut = [trial[twin_start:twin_end] for trial in norm_power]
            pwr = np.reshape(norm_power_cut, (1, trial_n*2000))[0]

            phase = [np.angle(compute_mwt(trial, fs=2000, peak_freq=f_pha, n=12)) for trial in R2_trials]      # high n for high f precision
            phase = [trial[epoch_start:epoch_end] for trial in phase]
            phase = [trial[twin_start:twin_end] for trial in phase]
            pha = np.reshape(phase, (1,trial_n*2000))[0]

            observed_pac = get_PAC(pha,pwr)

            pac_surr = []
            for i in range(n_perm):
                t_shifted_norm_power = get_timeshifted_null(norm_power)
                t_shifted_norm_power_cut = [trial[twin_start:twin_end] for trial in t_shifted_norm_power]
                pwr_surr = np.reshape(t_shifted_norm_power_cut, (1, trial_n*2000))[0]
                pac_surr.append(get_PAC(pha, pwr_surr))

            pacz = (observed_pac - np.average(pac_surr)) / np.std(pac_surr)

            end = datetime.datetime.now()
            print(f'f_pha, f_pwr pair ({f_pha}, {f_pow}) duration: {end - start}')

            pacz_by_power.append(pacz)
        pacz_by_phase.append(pacz_by_power)

    pacz_2d = np.array(pacz_by_phase).T

    np.save(fname, pacz_2d, allow_pickle=True)

    # freqs4pha = np.arange(3,13,0.1)
    # freqs4pow = np.arange(20,121)
    # levels = np.linspace(np.min(pacz_2d), np.max(pacz_2d),50)
    #
    # plt.figure(figsize=(5,5))
    # cs = plt.contourf(freqs4pha, freqs4pow, pacz_2d, levels=levels, cmap='jet')
    # cbar = plt.colorbar(cs,shrink=0.9)
    #
    # plt.xlabel('Frequency for phase (Hz)')
    # plt.ylabel('Frequency for power (Hz)')
    #
    # plt.show()

def norm_pwr(trials, freqs, n_dict, s_pre, s_post, epoch_len, pre_bline):
    '''
    wrapped spectrogram function for high frequncy analysis.
    '''
    if not trials == []:
        fs = 2000
        freqs = freqs

        mwt = [[compute_mwt(trial, fs=fs, peak_freq=f, n=n_dict[f]) for f in freqs] for trial in trials]
        pow = [(abs(np.array(epoch)))**2 for epoch in mwt]
        norm, epoch_len, pre_bline = baseline_norm(pow, s_pre, s_post, epoch_len, pre_bline)

        return norm

    else:
        return None


    epoch_len = 2
    pre_bline = 0.1
    freqs = np.arange(4,60,2)
    n_dict = dict([(i, 3+(i-1)*9/(len(range(1,121))-1)) for i in range(1,121)]) # scale n with freq

def ppcorr(R1_trials, R2_trials, freqs, n_dict, epoch_len, pre_bline):
    '''
    power-power correlation matrix
    '''
    R1_pwr = norm_pwr(R1_trials, freqs, n_dict, 1.25, 1.25, epoch_len, pre_bline)
    R2_pwr = norm_pwr(R2_trials, freqs, n_dict, 1.25, 1.25, epoch_len, pre_bline)

    R1_pwr = np.array(R1_pwr)[:,:,200:]
    R2_pwr = np.array(R2_pwr)[:,:,200:]

    R1_1d_byf = []
    for i in range(len(freqs)):
        R1_1d_byf.append(np.ravel(R1_pwr[:,i,:]))       # y axis

    R2_1d_byf = []
    for i in range(len(freqs)):
        R2_1d_byf.append(np.ravel(R2_pwr[:,i,:]))       # x axis

    ppcorr_by_R1R2 = []
    for pfc_fpow in R1_1d_byf:
        ppcorr_by_hpc = []
        for hpc_fpow in R2_1d_byf:
            r, p = stats.spearmanr(pfc_fpow, hpc_fpow)
            ppcorr_by_hpc.append(r)

        ppcorr_by_R1R2.append(ppcorr_by_hpc)

    return ppcorr_by_R1R2

def plot_ppcorr(ppcor_byR1R2, freqs, global_labels, local_label, r):
    '''
    global_labels: args.global_labels
    local_labels
    r: args.reg
    '''
    fig, ax = plt.subplots()
    im = ax.imshow(ppcor_byR1R2, origin='lower', cmap='bwr', vmin=-1, vmax=1)
    fig.colorbar(im, shrink=0.9)

    plt.setp(ax, xticks=range(len(freqs)), xticklabels=list(freqs))
    for label in ax.xaxis.get_ticklabels()[1::2]:
        label.set_visible(False)

    plt.setp(ax, yticks=range(len(freqs)), yticklabels=list(freqs))
    for label in ax.yaxis.get_ticklabels()[1::2]:
        label.set_visible(False)

    if r.lower() == 'pfc-pfc':
        r = 'pfc(r)-pfc(l)'

    ax.set_title(f'{global_labels} {local_label} \npower correlations 1s')
    ax.set_ylabel(f'{r.split("-")[0].upper()} frequency for power (Hz)')
    ax.set_xlabel(f'{r.split("-")[1].upper()} frequency for power (Hz)')
    fig.tight_layout()

    plt.savefig(f'{global_labels} {local_label} power correlations1s.png', dpi=600)


def PACcm(R1_trials, R2_trials, freqs4pha, freqs4pow, t_win, global_labels, local_label, epoch_len, s_pre, pre_bline, r):
    assert len(R1_trials) == len(R2_trials)
    trial_n = len(R1_trials)

    print('\ncompute PACs for raw comodulogram')

    fs = 2000
    epoch_samples = epoch_len * fs
    epoch_start = int((s_pre * fs ) - (epoch_samples / 2))          # slices symmetrically around timestamps
    epoch_end = int(epoch_samples) + epoch_start
    baseline_end = int((epoch_samples / 2) - (pre_bline * fs))

    def get_PAC(pha, pwr):
        PAC = abs(np.mean(pwr * np.exp(1j * pha)))
        return(PAC)

    twin_start = int(t_win[0]*fs) + 2000
    twin_end = int(t_win[1]*fs) + 2000

    pac_by_phase = []
    for f_pha in freqs4pha:

        pac_by_power = []
        for f_pow in freqs4pow:
            power = [abs(compute_mwt(trial, fs=2000, peak_freq=f_pow, n=12)) ** 2 for trial in R1_trials]     # high n for high f precision
            power = [trial[epoch_start:epoch_end] for trial in power]
            norm_power = [np.array(trial) / np.average(trial[:baseline_end]) for trial in power]
            norm_power = [trial[twin_start:twin_end] for trial in norm_power]
            pwr = np.reshape(norm_power, (1, trial_n*800))[0]


            phase = [np.angle(compute_mwt(trial, fs=2000, peak_freq=f_pha, n=12)) for trial in R2_trials]      # high n for high f precision
            phase = [trial[epoch_start:epoch_end] for trial in phase]
            phase = [trial[twin_start:twin_end] for trial in phase]
            pha = np.reshape(phase, (1,trial_n*800))[0]

            pac = get_PAC(pha, pwr)

            pac_by_power.append(pac)
        pac_by_phase.append(pac_by_power)

    pac_2d = np.array(pac_by_phase).T


    # time to plot

    plt.figure(figsize=(5,5))
    levels = np.linspace(np.min(pac_2d), np.max(pac_2d),50)
    cs = plt.contourf(freqs4pha, freqs4pow, pac_2d, levels=levels, cmap='jet')
    cbar = plt.colorbar(cs,shrink=0.9)

    title = f'{global_labels} {local_label}\nRaw PAC {int(t_win[0]*1000)}-{int(t_win[1]*1000)}ms'
    plt.title(title)
    plt.xlabel(f'{r.split("-")[1].upper()} Frequency for phase (Hz)')
    plt.ylabel(f'{r.split("-")[0].upper()} Frequency for power (Hz)')

    plt.savefig(title.replace('\n',' ') + '.png', dpi=600)

def lag_xcorr(R1_trials, R2_trials, lc, hc, order):
    fs=2000
    R1_amp = [abs(filter_hilbert(trial, lc, hc, fs, order))**2 for trial in R1_trials]
    R2_amp = [abs(filter_hilbert(trial, lc, hc, fs, order))**2 for trial in R2_trials]

    # the stationary timeseries
    R1_amp = np.concatenate([trial[2700:4701] for trial in R1_amp])

    # the sliding timeseries
    R2_amp = [trial[2500:4901] for trial in R2_amp]

    # generating sliding R2_timeseries
    xcorr_twindows = []
    for i in range(400):    # 400 because that encompasses -100 to +100 ms sliding time window
        R2_wind = np.concatenate([trial[0+i: 2001+i] for trial in R2_amp])   # cncat and caclcualte r for all trials at once

        assert np.shape(R1_amp) == np.shape(R2_wind)

        r, p = stats.spearmanr(R1_amp, R2_wind)
        xcorr_twindows.append((r, p))

    return xcorr_twindows

import pandas as pd
def plot_lag_xcorr(xcorr_twindows, title):
    xcorr_R = [r for r, p in xcorr_twindows]
    lags = np.linspace(-100,100,400)

    df_lags = pd.DataFrame()
    df_lags['lags'] = lags
    df_lags['r'] = xcorr_R
    pd.to_csv(f'{title}.csv')

    plt.figure(figsize=(6,4))
    plt.ylabel('Correlation')
    plt.xlabel('Lag (ms)')
    plt.axvline(x=0, color='k', ls='--', alpha=0.5)
    plt.title(title)

    if 'ChABC' in title:
        plt.plot(lags, xcorr_R, 'r', zorder=1)
    else:
        plt.plot(lags, xcorr_R, 'k', zorder=1)

    plt.scatter(lags[xcorr_R.index(np.max(xcorr_R))], np.max(xcorr_R), s=8**2,
    marker=(5,1), zorder=2, color='yellow', edgecolors='k', label=f'R={np.round(np.max(xcorr_R), 2)}\nat {np.round(lags[xcorr_R.index(np.max(xcorr_R))], 2)} ms')
    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, prop={'size':6})
    plt.tight_layout()
    plt.savefig(title.replace('\n', ' ')+'.png', dpi=600)
    #plt.show()


def lag_xcorr_bytrial(R1_trials, R2_trials, lc, hc, order):
    fs=2000
    R1_amp = [abs(filter_hilbert(trial, lc, hc, fs, order))**2 for trial in R1_trials]
    R2_amp = [abs(filter_hilbert(trial, lc, hc, fs, order))**2 for trial in R2_trials]

    # the stationary timeseries
    R1_amp = [trial[2700:4700] for trial in R1_amp]

    # the sliding timeseries
    R2_amp = [trial[2500:4901] for trial in R2_amp]

    xcorr_bytrial = []
    for R1_trial, R2_trial in zip(R1_amp, R2_amp):

        xcorr_trial  = []
        for i in range(401):

            R2_wind = R2_trial[0+i: 2000+i]

            assert np.shape(R1_trial) == np.shape(R2_wind)

            r, p = stats.spearmanr(R1_trial, R2_wind)

            xcorr_trial.append(r)

        xcorr_bytrial.append(xcorr_trial)

    return xcorr_bytrial

def plot_xcorr_bytrial(xcorr_bytrial, title):
    def _sort_trials_by_max(unsorted_bytrial):
        iimax = []
        for i, arr in enumerate(unsorted_bytrial):
            i_max = np.where(arr == np.max(arr))[0].item()
            iimax.append((arr, i_max))

        arr_imax_sort = sorted(iimax, key=lambda x: x[1])
        arr_sortbymax = np.array([arr for arr, i in arr_imax_sort])

        return arr_sortbymax

    def _norm_minmax(raw_xcorr_bytrial):
        norm = [(arr - np.min(arr)) / (np.max(arr) - np.min(arr)) for arr in raw_xcorr_bytrial]

        return np.array(norm)

    xcorr_bytrial = _sort_trials_by_max(xcorr_bytrial)
    xcorr_bytrial = _norm_minmax(xcorr_bytrial)

    f, ax = plt.subplots(figsize=(6,4))
    im = ax.imshow(xcorr_bytrial, cmap='jet', aspect='auto', vmin=0, vmax=1, interpolation='nearest')

    lag_ticks = np.linspace(0,400,9).tolist()
    lags = np.linspace(-100,100,9).astype(int)

    xcorr_bytrial = list(xcorr_bytrial)

    df_lags = pd.DataFrame()
    df_lags['trial'] = np.arange(len(xcorr_bytrial))
    df_lags['r'] = xcorr_bytrial
    pd.to_csv(f'{title}.csv')

    sys.exit()

    ax.set_xticks(lag_ticks)
    ax.set_xticklabels(lags)
    ax.axes.axvline(200,c='k',ls='--')
    ax.set_xlabel('Lag (ms)')
    ax.set_ylabel('trial')
    ax.set_title(title)

    #plt.savefig(title.replace('\n', ' ')+'.png', dpi=600)






def get_MI(norm_power, pha_bins):
    # calculate p(j)
    p = []
    n = len(pha_bins)

    for bin in pha_bins:
        avg_pwr = np.average(np.array(norm_power)[bin])
        p.append(avg_pwr)

    p = np.array(p) / np.sum(p)

    # Shannon entropy H(p)
    H = (-1)*np.sum([np.log(p_j)*p_j for p_j in p])

    # Kullback-Leibler distance KL
    KL = np.log(len(pha_bins)) - H

    # Tort et al 2008 Modulation Index
    MI = KL / np.log(len(pha_bins))

    return MI



def get_composite_signal_PAC_dist(trials_pha, trials_pow, s_pre, s_post, epoch_len, pre_bline, t0, t1):
    def _get_bin_indices(epoch):

        # bins of pi/50 are hard coded here.
        # for each pi/50 bin, generating an array of booleans where
        # phase value falls between bin_low and bin_high
        epoch_bin_bool = []
        for i in np.arange(-15,15):
            bin_low = (np.pi*i)/15
            bin_high = (np.pi*(i+1))/15
            bin1 = (bin_low < epoch) & (epoch < bin_high)

            epoch_bin_bool.append(bin1.tolist())

        # getting indices where True
        epoch_bin_indices = [np.argwhere(binned_epoch).tolist() for binned_epoch in epoch_bin_bool]

        # flattening the list of lists in our list of lists of lists... >:-(
        # lmao i didn't even need this but it's staying in here bc
        # this list (of lists) comprehension looks ridiculous
        epoch_bin_indices = [[index for lst in pha_bin for index in lst] for pha_bin in epoch_bin_indices]

        return epoch_bin_bool
    # theta phase time series
    fs = 2000
    epoch_samples = epoch_len * fs
    epoch_start = int((s_pre * fs ) - (epoch_samples / 2))          # slices symmetrically around timestamps
    epoch_end = int(epoch_samples) + epoch_start
    baseline_end = int((epoch_samples / 2) - (pre_bline * fs))      # baseline begins at the beginning of the trial.
                                                                    # and ends some number of ms right before timestamp
    if not epoch_end < int(s_post * fs) + int(s_pre * fs):
        print('Epoch end index beyond trial duration. Check your slice indices.')
        sys.exit(1)

    t_start = int(baseline_end+200+(t0*(1/1000)*fs))                      # t=0
    t_end = int(baseline_end+200+(t1*(1/1000)*fs))        # t end in ms

    pha_lc = 9
    pha_hc = 11
    pwr_lc = 70
    pwr_hc = 90

    pha = [np.angle(filter_hilbert(sig=trial, lc=pha_lc, hc=pha_hc, fs=fs, order=2)) for trial in trials_pha]
    pha = [trial[epoch_start:epoch_end] for trial in pha]   # a bit unnecessary but slicing the same way we do with power
    pha = [trial[t_start:t_end] for trial in pha]           # just doing it this way for consistency

    print(np.shape(pha))
    pha = [angle for trial in pha for angle in trial]
    pha_bins = _get_bin_indices(pha)

    # gamma power time series ### was 60-80
    pwr = [abs(filter_hilbert(sig=trial, lc=pwr_lc, hc=pwr_hc, fs=fs, order=6))**2 for trial in trials_pow]
    pwr = [trial[epoch_start:epoch_end] for trial in pwr]
    norm_power = [np.array(trial) / np.average(trial[:baseline_end]) for trial in pwr]
    norm_power = [trial[t_start:t_end] for trial in norm_power]
    norm_power = [amp for trial in norm_power for amp in trial]    # upack trials

    pac = get_PAC(np.array(pha), np.array(norm_power))
    print(pac)

    mi = get_MI(norm_power, pha_bins)
    print(mi)

    return pha_bins, norm_power, mi

def plot_avg_pwr_by_bin(pha_bins, norm_pow, global_label, local_label, t0, t1, pac):
    if 'ChABC' in global_label:
        c='red'
    elif 'Vehicle' in global_label:
        c='darkgray'
    else:
        c='white'

    avg_pwr_by_bin = []
    for bin in pha_bins:
        avg_pwr = np.average(np.array(norm_pow)[bin])
        avg_pwr_by_bin.append(avg_pwr)

    pi = np.pi
    x = np.linspace(-pi, pi, 30)
    vmax = 1.9

    f = plt.figure(figsize=(6,5))
    plt.bar(x, avg_pwr_by_bin, width=np.pi/15, color=c, edgecolor='k')
    plt.xticks(np.arange(-pi, pi+pi/2, step=(pi/2)), ['-Ï€','-Ï€/2','0','Ï€/2','Ï€'])
    plt.xlabel('Theta Phase, 5-7 Hz (rad)', fontsize=20)
    plt.ylabel('Relative Gamma Power,\n 70-90 Hz', fontsize=20)
    plt.ylim(0,vmax)
    plt.title(f'{global_label} {local_label}\n MI={np.round(pac,10)}, {t0}-{t1}ms')
    f.axes[0].tick_params(axis='both', labelsize=20)
    plt.tight_layout()
    plt.savefig(f'{global_label} {local_label} PAC {t0}-{t1}ms.png',dpi=600)
    return avg_pwr_by_bin

def plot_avg_pwr_by_bin_polar(pha_bins, norm_pow, global_label, local_label, t0, t1, pac):
    if 'ChABC' in global_label:
        c='red'
    elif 'Vehicle' in global_label:
        c='darkgray'
    else:
        c='white'

    avg_pwr_by_bin = []
    for bin in pha_bins:
        avg_pwr = np.average(np.array(norm_pow)[bin])
        avg_pwr_by_bin.append(avg_pwr)

    pi = np.pi
    theta = np.linspace(-pi, pi, 30)[:-1]
    radii = avg_pwr_by_bin[:-1]
    width = np.array([pi / 15] * 30)[:-1]
    ax = plt.subplot(projection='polar')
    ax.bar(theta, radii, width, color=c, edgecolor='black')
    ax.set_xticklabels(['0Ï€', 'Ï€/4', 'Ï€/2', '3Ï€/4', 'Ï€', '5Ï€/4', '3Ï€/2', '7Ï€/4'], size=18)
    ax.set_xticklabels(['0', 'Ï€/4', 'Ï€/2', '3Ï€/4', '-Ï€', '-3Ï€/4', '-Ï€/2', '-Ï€/4'], size=18)
    ax.set_rticks([0, 1, 2])  # Less radial ticks
    ax.set_yticklabels([0, 1, 2], size=14)

    plt.savefig(f'{global_label} {local_label} PAC {t0}-{t1}ms polar.png',dpi=600)
    return avg_pwr_by_bin

def zscore_PAC_pwrpha_dist(trials_pha, trials_pow, s_pre, s_post, epoch_len, pre_bline, t0, t1, global_label, local_label):
    def get_timeshifted_null(x_trials):
        '''
        x_trials is a list of trials where each trial is a list of timeseries data
        expects shape to be two dimensional

        takes trials from x_trials and randomly shifts them so that the trial starts at a random
        index ranging from 1:len(trial). trials are wrapped so that remaining trial data is concatenated
        to the end of the timeshifted trial.

        returns a list of time shifted trials.
        '''
        # draw n random integers from a uniform distribution ranging from 1 to l
        # where n is the number of trials and l is the length of each trial
        samp = np.random.uniform(1,np.shape(x_trials)[1], np.shape(x_trials)[0]).astype(int)

        # each trial is paired up with its own index from the random sample
        # trials are rearranged to begin at the random index and then concatenated
        # with remaining data at the end (wrapped)
        t_shifted_trials = [np.concatenate([trial[rand_t:],trial[:rand_t]]) for trial, rand_t in zip(x_trials, samp)]

        return t_shifted_trials

    # theta phase time series\
    fs = 2000
    epoch_samples = epoch_len * fs
    epoch_start = int((s_pre * fs ) - (epoch_samples / 2))          # slices symmetrically around timestamps
    epoch_end = int(epoch_samples) + epoch_start
    baseline_end = int((epoch_samples / 2) - (pre_bline * fs))      # baseline begins at the beginning of the trial.
                                                                    # and ends some number of ms right before timestamp
    if not epoch_end < int(s_post * fs) + int(s_pre * fs):
        print('Epoch end index beyond trial duration. Check your slice indices.')
        sys.exit(1)

    t_start = int(baseline_end+200+(t0*(1/1000)*fs))        # t=0
    t_end = int(baseline_end+200+(t1*(1/1000)*fs))          # t end in ms

    pha = [np.angle(filter_hilbert(sig=trial, lc=5, hc=7, fs=fs, order=3)) for trial in trials_pha]
    pha = [trial[epoch_start:epoch_end] for trial in pha]   # a bit unnecessary but slicing the same way we do with power
    pha = [trial[t_start:t_end] for trial in pha]           # just doing it this way for consistency

    print(np.shape(pha))
    pha_1D = [angle for trial in pha for angle in trial] # flatten to 1D

    # gamma power time series # changed from 60-80 to 50-70
    pwr = [abs(filter_hilbert(sig=trial, lc=50, hc=70, fs=fs, order=6))**2 for trial in trials_pow]
    pwr = [trial[epoch_start:epoch_end] for trial in pwr]
    norm_power = [np.array(trial) / np.average(trial[:baseline_end]) for trial in pwr]
    norm_pwr = [trial[t_start:t_end] for trial in pwr]
    norm_power = [amp for trial in norm_pwr for amp in trial] # flatten to 1D

    observed_pac = get_PAC(np.array(pha_1D), np.array(norm_power))
    print(observed_pac)

    # time to shuffle data, generate null distribution
    null_dist_pac = []
    for i in range(10000):
        # randomly shift phase
        null_pha = get_timeshifted_null(pha)
        null_pha_1D = [angle for trial in null_pha for angle in trial]

        # randomly shift power
        null_pwr = get_timeshifted_null(norm_pwr)
        null_pwr_1D = [amp for trial in null_pwr for amp in trial]

        # compute surrogate PAC
        null_pac = get_PAC(np.array(null_pha_1D), np.array(null_pwr_1D))
        null_dist_pac.append(null_pac)

        print(f'{np.round(i/10000*100,2)}% done', end='\r')


    zscore = (observed_pac - np.mean(null_dist_pac)) / np.std(null_dist_pac)
    gte = np.sum([(n >= observed_pac) for n in null_dist_pac])
    p = gte / 10000
    print(f'p={p}')
    print(f'z={zscore}\n\n')


    if 'Vehicle' in global_label:
        c='darkgray'
    elif 'ChABC' in global_label:
        c='red'


    f = plt.figure(figsize=(6,5))
    plt.hist(np.array(null_dist_pac), edgecolor='k', color=c, bins=30)
    plt.axvline(x=observed_pac, color='k', label=f'PAC={np.round(observed_pac,2)}\n z={np.round(zscore,2)}')
    #plt.legend(bbox_to_anchor=(1.02, 0.5), loc='center left', borderaxespad=0)
    plt.legend(loc='upper right', handlelength=1.0)
    plt.xlabel('Null PAC values, 10000 permutations', fontsize=18)
    plt.ylabel('Counts', fontsize=18)
    f.axes[0].tick_params(axis='both', labelsize=16)
    plt.title(f'{global_label} {local_label}\n Null PAC distribution, {t0}-{t1}ms', fontsize=18)
    plt.tight_layout()
    plt.savefig(f'{global_label} {local_label} Null PAC distribution, {t0}-{t1}ms.png', dpi=600)





    #return pha_bins, norm_power, pac

def zscore_MI_pwrpha_dist(trials_pha, trials_pow, s_pre, s_post, epoch_len, pre_bline, t0, t1, global_label, local_label):
    def get_timeshifted_null(x_trials):
        '''
        x_trials is a list of trials where each trial is a list of timeseries data
        expects shape to be two dimensional

        takes trials from x_trials and randomly shifts them so that the trial starts at a random
        index ranging from 1:len(trial). trials are wrapped so that remaining trial data is concatenated
        to the end of the timeshifted trial.

        returns a list of time shifted trials.
        '''
        # draw n random integers from a uniform distribution ranging from 1 to l
        # where n is the number of trials and l is the length of each trial
        samp = np.random.uniform(1,np.shape(x_trials)[1], np.shape(x_trials)[0]).astype(int)

        # each trial is paired up with its own index from the random sample
        # trials are rearranged to begin at the random index and then concatenated
        # with remaining data at the end (wrapped)
        t_shifted_trials = [np.concatenate([trial[rand_t:],trial[:rand_t]]) for trial, rand_t in zip(x_trials, samp)]

        return t_shifted_trials

    def _get_bin_indices(epoch):

        # bins of pi/50 are hard coded here.
        # for each pi/50 bin, generating an array of booleans where
        # phase value falls between bin_low and bin_high
        epoch_bin_bool = []
        for i in np.arange(-15,15):
            bin_low = (np.pi*i)/15
            bin_high = (np.pi*(i+1))/15
            bin1 = (bin_low < epoch) & (epoch < bin_high)

            epoch_bin_bool.append(bin1.tolist())

        # getting indices where True
        epoch_bin_indices = [np.argwhere(binned_epoch).tolist() for binned_epoch in epoch_bin_bool]

        # flattening the list of lists in our list of lists of lists... >:-(
        # lmao i didn't even need this but it's staying in here bc
        # this list (of lists) comprehension looks ridiculous
        epoch_bin_indices = [[index for lst in pha_bin for index in lst] for pha_bin in epoch_bin_indices]

        return(epoch_bin_bool)

    # theta phase time series\
    fs = 2000
    epoch_samples = epoch_len * fs
    epoch_start = int((s_pre * fs ) - (epoch_samples / 2))          # slices symmetrically around timestamps
    epoch_end = int(epoch_samples) + epoch_start
    baseline_end = int((epoch_samples / 2) - (pre_bline * fs))      # baseline begins at the beginning of the trial.
                                                                    # and ends some number of ms right before timestamp
    if not epoch_end < int(s_post * fs) + int(s_pre * fs):
        print('Epoch end index beyond trial duration. Check your slice indices.')
        sys.exit(1)

    t_start = int(baseline_end+200+(t0*(1/1000)*fs))        # t=0
    t_end = int(baseline_end+200+(t1*(1/1000)*fs))          # t end in ms

    pha_lc = 5
    pha_hc = 7
    pwr_lc = 70
    pwr_hc = 90

    print(f'freq for pha: {pha_lc}-{pha_hc}')
    print(f'freq for pwr: {pwr_lc}-{pwr_hc}')

    pha = [np.angle(filter_hilbert(sig=trial, lc=pha_lc, hc=pha_hc, fs=fs, order=2)) for trial in trials_pha]
    pha = [trial[epoch_start:epoch_end] for trial in pha]   # a bit unnecessary but slicing the same way we do with power
    pha = [trial[t_start:t_end] for trial in pha]           # just doing it this way for consistency
    pha_1D = [angle for trial in pha for angle in trial]
    pha_bins = _get_bin_indices(pha_1D)

    # gamma power time series # changed from 60-80 to 50-70
    pwr = [abs(filter_hilbert(sig=trial, lc=pwr_lc, hc=pwr_hc, fs=fs, order=6))**2 for trial in trials_pow]
    pwr = [trial[epoch_start:epoch_end] for trial in pwr]
    norm_power = [np.array(trial) / np.average(trial[:baseline_end]) for trial in pwr]
    norm_pwr = [trial[t_start:t_end] for trial in norm_power]
    norm_power = [amp for trial in norm_pwr for amp in trial] # flatten to 1D


    observed_mi = get_MI(norm_power, pha_bins)
    print(observed_mi)

    # # time to shuffle data, generate null distribution
    # null_dist_mi = []
    # for i in range(10000):
    #     # randomly time shift phase
    #     null_pha = get_timeshifted_null(pha)
    #     null_pha_1D = [angle for trial in null_pha for angle in trial]
    #     null_pha_bins = _get_bin_indices(null_pha_1D)
    #
    #     # random time shift pwr
    #     null_pwr = get_timeshifted_null(norm_pwr)
    #     null_pwr_1D = [amp for trial in null_pwr for amp in trial]
    #
    #     null_mi = get_MI(null_pwr_1D, null_pha_bins)
    #     null_dist_mi.append(null_mi)
    #     print(f'{np.round(i/10000*100,2)}% done', end='\r')

    null_dist_mi = []
    for i in range(10000):
        random.shuffle(pha)
        null_pha = pha
        null_pha_1D = [angle for trial in null_pha for angle in trial]
        null_pha_bins = _get_bin_indices(null_pha_1D)

        random.shuffle(norm_pwr)
        null_pwr = norm_pwr
        null_pwr_1D = [amp for trial in null_pwr for amp in trial]

        null_mi = get_MI(null_pwr_1D, null_pha_bins)
        null_dist_mi.append(null_mi)
        print(f'{np.round(i/10000*100,2)}% done', end='\r')

    zscore = (observed_mi - np.mean(null_dist_mi)) / np.std(null_dist_mi)
    print('\nzscore: ',zscore,'\n')


    if 'Vehicle' in global_label:
        c='darkgray'
    elif 'ChABC' in global_label:
        c='red'


    f = plt.figure(figsize=(6,5))
    plt.hist(np.array(null_dist_mi), edgecolor='k', color=c, bins=30)
    plt.axvline(x=observed_mi, color='k', label=f'MI={np.round(observed_mi,2)}\n z={np.round(zscore,2)}')
    #plt.legend(bbox_to_anchor=(1.02, 0.5), loc='center left', borderaxespad=0)
    plt.legend(loc='upper right', handlelength=1.0)
    plt.xlabel('Null MI values, 10000 permutations', fontsize=18)
    plt.ylabel('Counts', fontsize=18)
    f.axes[0].tick_params(axis='both', labelsize=16)
    plt.title(f'{global_label} {local_label}\n Null MI distribution, {t0}-{t1}ms', fontsize=18)
    plt.tight_layout()
    plt.savefig(f'{global_label} {local_label} Null MI distribution, {t0}-{t1}ms.png', dpi=600)




def norm_power_bytrial(trials, freqs, n_dict, s_pre, s_post, epoch_len, pre_bline):
    '''
    wrapped spectrogram function for high frequncy analysis.
    '''
    def _baseline_norm(Tr_pow, s_pre, s_post, epoch_len, pre_bline):
        # calculate epoch and baseline indices from s_pre, s_post. I hate always doing
        # this by hand so let's just write it in terms of s_pre, s_post which are given
        '''
        short fn to slice baseline and epoch from power extracted from mwt.
        For each f (defined by params of convolution) takes the average value in
        baseline slice and divides every value in epoch slice by that value.
        '''
        def _norm(means, baseline_mean, db=False):
            '''
            small norm fn for baseline_norm()
            '''
            mean_norm = []
            for i in range(len(baseline_mean)):
                if db == False:
                    norm = means[i] / baseline_mean[i]
                elif db == True:
                    norm = 10*np.log10(means[i] / baseline_mean[i])
                mean_norm.append(norm)

            return(np.array(mean_norm))

        fs = 2000
        epoch_samples = epoch_len * fs                              # the duration of our entire epoch we want to cut out
        epoch_start = int((s_pre * fs ) - (epoch_samples / 2))      # the sample (index) where our epoch begins
        epoch_end = int(epoch_samples) + epoch_start                # the sample (index) where our epoch ends (the start i + total epoch duration)
        baseline_end = int((epoch_samples / 2) - (pre_bline * fs))  # the sample, i where [0:i] will be used for baselining

        if not epoch_end < int(s_post * fs) + int(s_pre * fs):
            print('Epoch end index beyond trial duration. Check your slice indices.')
            sys.exit(1)

        # cutting off our "wings"
        Trpow = [[fs[epoch_start:epoch_end] for fs in epoch] for epoch in Tr_pow]

        # defining our baseline epochs
        Trbase = [[fs[0:baseline_end] for fs in epoch] for epoch in Trpow]

        # defining our signal epochs
        Trsig = [[fs[baseline_end:] for fs in epoch] for epoch in Trpow]
        # means of baselines
        Trbase_fmean = [np.array([freq.mean() for freq in epoch]) for epoch in Trbase]

        # normlize to baseline
        Tr_norm = []
        for i in range(len(Trsig)):
            trial_norm = _norm(Trsig[i], Trbase_fmean[i])
            Tr_norm.append(trial_norm)

        return Tr_norm, epoch_len, pre_bline

    if not trials == []:
        fs = 2000
        freqs = freqs

        mwt = [[compute_mwt(trial, fs=fs, peak_freq=f, n=n_dict[f]) for f in freqs] for trial in trials]
        pow = [(abs(np.array(epoch)))**2 for epoch in mwt]

        norm, epoch_len, pre_bline = baseline_norm(pow, s_pre, s_post, epoch_len, pre_bline)

        return norm

    else:
        return None

def corr_pwr(R1_norm, R2_norm):
    def _fisherz(r):
        return 0.5 * np.log((1+r)/(1-r))

    r_bytrial = []
    for R1_trial, R2_trial in zip(R1_norm, R2_norm):

        r_byf = []
        for R1_f, R2_f in zip(R1_trial, R2_trial):
            r, p = stats.spearmanr(R1_f[200:], R2_f[200:])
            r_fz = _fisherz(r)
            r_byf.append(r_fz)

        r_bytrial.append(r_byf)

    return r_bytrial



def main():
    pass

if __name__ == '__main__':
    main()
